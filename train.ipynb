{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb200416-d927-4302-b6b2-a0bfe159c321",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f1ae964-36e4-4a3a-8109-306ce7cac387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that. \n",
    "# you prolly won't need this cell but running it won't hurt anything either\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426337be-2a22-41cb-8e46-494ed823c037",
   "metadata": {},
   "source": [
    "# Instantiate a brand new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "209f309f-df73-4183-9d1c-a4fb834e825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelConfig(dim=256, device='cpu', tokenizer='bpe', vocab_len=8192, num_layers=4, second_resid_norm=False, mlp_hidden_mult=4, mlp_bias=False, mlp_nonlinearity='SiLU', mlp_gated=True, num_q_heads=2, num_kv_heads=1, head_dim=128, theta=10000, max_seq_len=512, scale_first_resid=True, norm_type='LayerNorm', norm_affine=True, norm_bias=True, eps=1e-06, max_batch_size=1) \n",
      "\n",
      " TrainConfig(weight_decay=0.05, batch_size=32, max_iters=50, eval_interval=10, eval_samples=1, checkpoint_interval=None, lr_init=1e-06, lr_max=0.1, lr_min=0.001, warmup_iters=0, final_flat_iters=5, anneal_type='cos', num_restarts=3, T_mult=2)\n",
      "\n",
      "4984.064K parameters\n",
      "\n",
      "Model(\n",
      "  (token_embedder): Embedding(8195, 256)\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x Layer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): MQA(\n",
      "        (Wq): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (Wk): Linear(in_features=256, out_features=128, bias=False)\n",
      "        (Wv): Linear(in_features=256, out_features=128, bias=False)\n",
      "        (Wo): Linear(in_features=256, out_features=256, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wup): Linear(in_features=256, out_features=682, bias=False)\n",
      "        (Wgate): Linear(in_features=256, out_features=682, bias=False)\n",
      "        (Wdown): Linear(in_features=682, out_features=256, bias=False)\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# config file\n",
    "from config import ModelConfig, TrainConfig\n",
    "cfg = ModelConfig()\n",
    "tcfg = TrainConfig()\n",
    "print(cfg, '\\n\\n', tcfg)\n",
    "\n",
    "# import the tokenizer specified by cfg\n",
    "from tools import import_from_nested_path\n",
    "imported_objects = import_from_nested_path(['tokenizers', cfg.tokenizer], 'tokenizer', ['get_tokenizer'])\n",
    "get_tokenizer = imported_objects.get('get_tokenizer')\n",
    "tokenizer = get_tokenizer(size = cfg.vocab_len)\n",
    "\n",
    "# model modules\n",
    "from modules.model import Model\n",
    "model = Model(cfg).to(cfg.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(f'\\n{sum(p.numel() for p in model.parameters())/1e3}K parameters\\n')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287e188-9a0d-47da-9d61-8885b29ba25d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6926710-6418-47c1-ba9f-ac9d63c092d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tools import get_data_loader\n",
    "from train import scheduler_lambda, train\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = tcfg.lr_max, weight_decay = tcfg.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler_lambda)\n",
    "\n",
    "train_data_loader = get_data_loader(batch_size=tcfg.batch_size, split='train')\n",
    "test_data_loader = get_data_loader(batch_size=tcfg.batch_size, split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a85b7902-ee37-4963-b263-f28e72ecaf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # set to true if you'd like to see a graph of the learning rate schedule\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Generate learning rate values\n",
    "    lrs = [scheduler_lambda(i) for i in range(tcfg.max_iters)]\n",
    "    \n",
    "    # Plot the learning rates\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(lrs, label='Learning Rate')\n",
    "    plt.title('Learning Rate Schedule')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c471d7-d40a-48b9-ac3f-82371e85c24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▌                                                                            | 1/50 [00:10<08:54, 10.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0000: lr 0.010000, train loss 205.7613, val loss 208.0311, ppl inf, time elapsed: 6.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▏                                                                       | 4/50 [01:40<19:19, 25.20s/it]\n"
     ]
    }
   ],
   "source": [
    "model, optimizer, log_data = train(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    cfg, \n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    tcfg, \n",
    "    train_data_loader,\n",
    "    test_data_loader,\n",
    "    #log_data: list = None, \n",
    "    #detect_anomoly = False # use if you're getting crazy errors about a the gradient being broken\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc1adb-a1fd-4f0e-8666-26b2a99a0e54",
   "metadata": {},
   "source": [
    "# inference test before you decide to save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "565eeefb-ce04-44c5-a0b7-81a1a75786c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a little girl named Amy. She loved to play outside with her mom. One day, she saw a big boat in the park. It was a cat, but it was very big and had a toy. Amy was very happy and said, \"Wow! It's a pretty peaceful snake with my friends.\" Amy was very happy to be there.\n",
      "Amy and her friends walked to the kitchen. They saw the rain and the cat played with the ball. Amy was so happy to see the little bird playing with her. They knew that it was a fun day for the little bird to make it happy.\n"
     ]
    }
   ],
   "source": [
    "from inference import generate\n",
    "prompt = \"Once upon a time\"\n",
    "model.eval()\n",
    "output = generate(\n",
    "    prompt, \n",
    "    model, \n",
    "    tokenizer,\n",
    "    #max_gen_len = 512,\n",
    "    temperature = 0.7,\n",
    "    #memory_saver_div = 8,\n",
    "    #top_p = 0.9,\n",
    "    #top_k = 32,\n",
    ")\n",
    "model.train()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf4ecb-0313-43a2-a35b-1f6c6e348427",
   "metadata": {},
   "source": [
    "# Saving your final model\n",
    "if `tcfg.checkpoint_interval != None` then checkpoints have already been saved\n",
    "\n",
    "you DO still need to do this even if you had been saving checkpoints; the final state has not yet been saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1c31388-e8b5-48c6-b057-e185586b0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import save_model\n",
    "save_model(model, cfg, tcfg, log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303e3a1e-85cb-4fe6-a85b-1d3ec773eb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
