{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2351bdee-ae93-4140-a11a-aacfb1254bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that. \n",
    "# you prolly won't need this cell but running it won't hurt anything either\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) \n",
    "\n",
    "# so that we can import `tools.py`\n",
    "# Navigate two directories up from the current working directory\n",
    "module_path = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "# Add this directory to sys.path if it's not already included\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eba617fb-d786-4169-8d39-2a97145c9245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc49c760-d96b-420f-8b05-4edd5fd4640d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83066e04393749e1acb7b853316733a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/23781 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a66369808444259ae519db96f86df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.33 s, sys: 2.4 s, total: 9.73 s\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# loading a sample from the dataset specified in config, in this case 'HuggingFaceFW/fineweb'\n",
    "from tools import get_data_loaders\n",
    "train_data_loader, val_data_loader = get_data_loaders(\n",
    "    'HuggingFaceFW/fineweb', \n",
    "    batch_size=100, \n",
    "    streaming=True,\n",
    "    subset_name = \"CC-MAIN-2024-10\" # using a different subset of fineweb from what we'll be training the model on\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ed44a9-bd6c-4042-92b1-9492927e98c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 That is to say, it’s a major departure from the authoritative air and hard-edged beauty of Singapore – in the best possible way.\n",
      "The most eventful thing we did in Manila was buy our suits for Trevor’s wedding. I’ll get to that in a follow-up post. But there were some remarkable charms to this place, despite its rough exterior.\n",
      "For instance, we stayed at a great little hotel/hostel, about a quarter mile from the ocean, called the Buoy. The owner was a domineering asshole, but fortunately we only met him once. His staff, on the other hand, were a delight. The all-female staff of the attached restaurant/bar were especially sweet and great to be around, and this led us to favor the hotel for our near-constant drinking during the few days we were there. We’d sit and drink buckets of beer, and work on this stupid little website we have (maybe you’ve heard of it), and play pool, and hang out with the Buoy girls. We spent a lot of time that way. (Hi girls!)\n",
      "I’m pleased to report that we also reconnected with our old college pal Cat “I just keep getting prettier” Tolentino. Her family hails from ’round Manila way, so she happened to be there on a vacation. We met a number of her family members, whose names and relationships I forgot as soon as I learned them. Cat was also the one who recommended that we go to Boracay, which turned out to be one of the crazier experiences of our whole trip (post forthcoming).\n",
      "We also spent some time hanging out with an American ex-military guy named Jack, now living in Cebu, who was absolutely insane. And we were lucky for it – he took us around to places we never would have seen, and told us stories that kept us laughing for days afterward. Jack, if you’re reading this, it was a real pleasure and we hope to see you again down the road.\n",
      "Enough of the sentimental stuff. Some other distinctions of note for Manila:\n",
      "- Worst food – Vince and I joked that the Filipino national dish seemed to be an unadorned boiled hotdog on a plate with some rice. (The burger I had at the place we went with Cat was excellent, but that was the exception. And that sucker was like $15.)\n",
      "- Most McDonald’s eaten (see #1 above)\n",
      "- Most beer ordered/consumed in bucket form\n",
      "- Best Wingman Ever (title awarded to Franklin Page). I’m not going to write the story out here. You’ll just have to ask us sometime.\n",
      "The lovely Buoy Hostel.\n",
      "Our Digital Nomad Mobile Office™\n",
      "One of our favorite Buoy staff members, Merlyn.\n",
      "Merlyn and Cathyrine – our best Buoy friends.\n",
      "Random hockey game in a shopping mall that Jack took us to.\n",
      "Headed off to Taipei…\n",
      "CPU times: user 123 ms, sys: 37.7 ms, total: 161 ms\n",
      "Wall time: 518 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batch = next(train_data_loader)\n",
    "print(len(batch), batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2133207-6ee0-4f47-8018-da1a867c5136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310776\n",
      "That is to say, it’s a major departure from the authoritative air and hard-edged beauty of Singapore\n",
      "[84, 104, 97, 116, 32, 105, 115, 32, 116, 111, 32, 115, 97, 121, 44, 32, 105, 116, 226, 128, 153, 115, 32, 97, 32, 109, 97, 106, 111, 114, 32, 100, 101, 112, 97, 114, 116, 117, 114, 101, 32, 102, 114, 111, 109, 32, 116, 104, 101, 32, 97, 117, 116, 104, 111, 114, 105, 116, 97, 116, 105, 118, 101, 32, 97, 105, 114, 32, 97, 110, 100, 32, 104, 97, 114, 100, 45, 101, 100, 103, 101, 100, 32, 98, 101, 97, 117, 116, 121, 32, 111, 102, 32, 83, 105, 110, 103, 97, 112, 111]\n",
      "That is to say, it’s a major departure from the authoritative air and hard-edged beauty of Singapore\n"
     ]
    }
   ],
   "source": [
    "# turn it into one string instead of a list of strings\n",
    "combined_string = '\\n\\n'.join(batch)\n",
    "\n",
    "# Convert the string to bytes\n",
    "combined_bytes = combined_string.encode('utf-8')\n",
    "print(len(combined_bytes))\n",
    "\n",
    "print(combined_string[:100])\n",
    "print([b for b in combined_bytes[:100]])\n",
    "print(combined_bytes.decode('utf-8')[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca80d95-7491-4ffb-9bbc-63e521306590",
   "metadata": {},
   "source": [
    "# Regex\n",
    "this is a pre-processing stage where we set the rules for what types of characters are allowed to be merged together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5da62b54-17dd-48a0-bab4-f9b1fd07413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d3dad59-9791-4588-96c3-134a2d8390a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't ask me the specifics of how this plays out, i just know it's what they used for GPT4\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "# if you want to mess around with building your own tokenizer, then ^this string is one of the things to mess around with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c04f316b-21a9-4bdc-a7ed-c622eb1b4e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex.Regex(\"'(?i:[sdmt]|ll|ve|re)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?+\\\\p{L}+|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]++[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]|\\\\s+(?!\\\\S)|\\\\s+\", flags=regex.V0)\n"
     ]
    }
   ],
   "source": [
    "compiled_pattern = re.compile(GPT4_SPLIT_PATTERN, re.UNICODE)\n",
    "print(compiled_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c183a0fc-400d-41ed-9272-1d7a96e08247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308521 59869\n"
     ]
    }
   ],
   "source": [
    "# split the text up into text chunks\n",
    "text_chunks = re.findall(compiled_pattern, combined_string)\n",
    "print(len(combined_string), len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dad461b-9959-463e-9a87-8934400ecb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Viewing Single Post From: Spoilers for the Week of February 11th|\n",
      "|Lil||Feb 1 2013, 09:58 AM|\n",
      "Don't\n",
      "['|Viewing', ' Single', ' Post', ' From', ':', ' Spoilers', ' for', ' the', ' Week', ' of', ' February', ' ', '11', 'th', '|\\n', '|Lil', '||', 'Feb', ' ', '1', ' ', '201', '3', ',', ' ', '09', ':', '58', ' AM', '|\\n', 'Don', \"'t\", ' care', ' about', ' Chloe', '/Taniel', '/Jen', '-Jen', '.', ' Don', \"'t\", ' care', ' about', ' Sami', ',', ' really', ',', ' but', ' hoping', ' that', ' we', ' get', ' some', ' good', ' \"', 'SAMANTHA', ' GENE', '!!\"', ' Marlena', ' Death', '-Stares', ' out', ' of', ' it', '.', ' And', ' \"', 'newfound', '\"', ' feelings', '.', ' Please', '.', ' If', ' only', '.\\n', 'STEFANO', '!!', ' STEFANO', ',', ' STEFANO', ',', ' STEFANO', '!!!!', ' :', 'cheer', ':\\n', '|Spoilers', ' for', ' the', ' Week', ' of', ' February', ' ', '11', 'th', ' ·', ' DAYS', ':', ' News']\n"
     ]
    }
   ],
   "source": [
    "print(combined_string[:100])\n",
    "print(text_chunks[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faa9c183-5eb1-4b32-a5f7-3e5813b1e100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150958 [[124, 86, 105, 101, 119, 105, 110, 103], [32, 83, 105, 110, 103, 108, 101], [32, 80, 111, 115, 116], [32, 70, 114, 111, 109], [58], [32, 83, 112, 111, 105, 108, 101, 114, 115], [32, 102, 111, 114], [32, 116, 104, 101], [32, 87, 101, 101, 107], [32, 111, 102], [32, 70, 101, 98, 114, 117, 97, 114, 121], [32], [49, 49], [116, 104], [124, 10], [124, 76, 105, 108], [124, 124], [70, 101, 98], [32], [49], [32], [50, 48, 49], [51], [44], [32], [48, 57], [58], [53, 56], [32, 65, 77], [124, 10], [68, 111, 110], [39, 116], [32, 99, 97, 114, 101], [32, 97, 98, 111, 117, 116], [32, 67, 104, 108, 111, 101], [47, 84, 97, 110, 105, 101, 108], [47, 74, 101, 110], [45, 74, 101, 110], [46], [32, 68, 111, 110], [39, 116], [32, 99, 97, 114, 101], [32, 97, 98, 111, 117, 116], [32, 83, 97, 109, 105], [44], [32, 114, 101, 97, 108, 108, 121], [44], [32, 98, 117, 116], [32, 104, 111, 112, 105, 110, 103], [32, 116, 104, 97, 116], [32, 119, 101], [32, 103, 101, 116], [32, 115, 111, 109, 101], [32, 103, 111, 111, 100], [32, 34], [83, 65, 77, 65, 78, 84, 72, 65], [32, 71, 69, 78, 69], [33, 33, 34], [32, 77, 97, 114, 108, 101, 110, 97], [32, 68, 101, 97, 116, 104], [45, 83, 116, 97, 114, 101, 115], [32, 111, 117, 116], [32, 111, 102], [32, 105, 116], [46], [32, 65, 110, 100], [32, 34], [110, 101, 119, 102, 111, 117, 110, 100], [34], [32, 102, 101, 101, 108, 105, 110, 103, 115], [46], [32, 80, 108, 101, 97, 115, 101], [46], [32, 73, 102], [32, 111, 110, 108, 121], [46, 10], [83, 84, 69, 70, 65, 78, 79], [33, 33], [32, 83, 84, 69, 70, 65, 78, 79], [44], [32, 83, 84, 69, 70, 65, 78, 79], [44], [32, 83, 84, 69, 70, 65, 78, 79], [33, 33, 33, 33], [32, 58], [99, 104, 101, 101, 114], [58, 10], [124, 83, 112, 111, 105, 108, 101, 114, 115], [32, 102, 111, 114], [32, 116, 104, 101], [32, 87, 101, 101, 107], [32, 111, 102], [32, 70, 101, 98, 114, 117, 97, 114, 121], [32], [49, 49], [116, 104], [32, 194, 183], [32, 68, 65, 89, 83], [58], [32, 78, 101, 119, 115]]\n"
     ]
    }
   ],
   "source": [
    "# input text preprocessing\n",
    "ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks] \n",
    "ids_backup = ids # saving this for later just to see how much compression we get\n",
    "print(len(ids), ids[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b53373-b33d-4eba-b640-a50e3334045a",
   "metadata": {},
   "source": [
    "so this regex just splits the text up into all the token ids that are allowed to be merged, meaning that the regex output we saw above is an upper limit on the tokens that we could end up with if we get a large enough vocabulary, rather than a starting point. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e216a550-4bbb-4d8e-ba57-25af48a9d65b",
   "metadata": {},
   "source": [
    "# BPE tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba69cd54-8390-4355-881a-cd2f233922dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1024 - 3 # the desired final vocabulary size. -3 for the three special tokens bos, eos, & pad\n",
    "num_merges = vocab_size - 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0e7fcb6-5927-45b9-b6a9-23ce1df8f312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids, counts=None):\n",
    "    \"\"\"\n",
    "    Given a list of integers, return a dictionary of counts of consecutive pairs\n",
    "    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
    "    Optionally allows to update an existing dictionary of counts\n",
    "    \"\"\"\n",
    "    counts = {} if counts is None else counts\n",
    "    for pair in zip(ids, ids[1:]): # iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    \"\"\"\n",
    "    In the list of integers (ids), replace all consecutive occurrences\n",
    "    of pair with the new integer token idx\n",
    "    Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n",
    "    \"\"\"\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if not at the very last position AND the pair matches, replace it\n",
    "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba933bfb-026c-47f7-8fff-42868bf5818c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 765/765 [02:20<00:00,  5.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# now let's actually do it\n",
    "merges = {} # (int, int) -> int\n",
    "for i in tqdm(range(num_merges)):\n",
    "    # count the number of times every consecutive pair appears\n",
    "    stats = {}\n",
    "    for chunk_ids in ids:\n",
    "        # passing in stats will update it in place, adding up counts\n",
    "        get_stats(chunk_ids, stats)\n",
    "    # find the pair with the highest count\n",
    "    pair = max(stats, key=stats.get)\n",
    "    # mint a new token: assign it the next available id\n",
    "    idx = 256 + i\n",
    "    # replace all occurrences of pair in ids with idx\n",
    "    ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]\n",
    "    # save the merge\n",
    "    merges[pair] = idx\n",
    "    #print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} had {stats[pair]} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ea99684-f232-4a8f-80f4-48b841633ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original length: 735934\n",
      "ids length: 301100\n",
      "compression ratio: 2.44X\n"
     ]
    }
   ],
   "source": [
    "og = sum([len(t) for t in ids_backup])\n",
    "new = sum([len(t) for t in (ids)])\n",
    "print(\"original length:\", og)\n",
    "print(\"ids length:\", new)\n",
    "print(f\"compression ratio: {og / new:.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bffaa27-acf6-4173-8f64-44f8def4a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124, 86, 813, 276] |Viewing\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "    # given ids (list of integers), return Python string\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text\n",
    "\n",
    "print(ids[0], decode(ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad46ab33-fa0a-4c88-8779-27387f3fceb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79, 969, 498, 261, 257, 602, 44]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    # given a string, return list of integers (the tokens)\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while len(tokens) >= 2:\n",
    "        stats = get_stats(tokens)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            break # nothing else can be merged\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "\n",
    "print(encode(\"Once upon a time,\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcfc996d-fc57-469d-9a25-bc441cc4242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the tokenizers directory exists\n",
    "if not os.path.exists('./models'):\n",
    "    os.makedirs('./models')\n",
    "\n",
    "# Prepare the tokenizer data to be saved\n",
    "tokenizer_data = {'merges': merges}\n",
    "\n",
    "# Save the tokenizer data using pickle\n",
    "with open(f'./models/{vocab_size}.model', 'wb') as f:\n",
    "    pickle.dump(tokenizer_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db232b16-94c5-4a41-ba7f-63c26bcfff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a pre-existing tokenizer and trimming it down to a smaller size\n",
    "# i basically ran this cell and then one above it multiple times until i got to the smallest possible size (512 - 3 = 509)\n",
    "vocab_size = ((vocab_size + 3) // 2) - 3 # the -3's account for our special tokens\n",
    "merges = {k: v for k, v in merges.items() if v < vocab_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "030522f4-ad67-4200-8598-261a9e1fa744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import load_tokenizer_data, BPE_Tokenizer\n",
    "vocab_size = 1024\n",
    "tokenizer_data = load_tokenizer_data(f'models/{vocab_size-3}.model')\n",
    "tokenizer = BPE_Tokenizer(tokenizer_data['merges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "857e3d5b-85b9-4ce7-bd41-f1f8de91cb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', ' b', 'est', ' and', ' most', ' re', 'l', 'i', 'able', ' for', 'm', ' of', ' re', 'se', 'arch', ' is', ' the', ' d', 'ou', 'ble', '-', 'b', 'l', 'ind', ',', ' pl', 'ace', 'b', 'o', '-', 'c', 'ont', 'ro', 'll', 'ed', ' stud', 'y', '.']\n",
      "[1021, 442, 277, 394, 288, 790, 303, 108, 105, 570, 320, 109, 287, 303, 362, 739, 326, 263, 293, 278, 808, 45, 98, 108, 523, 44, 469, 456, 98, 111, 45, 99, 783, 302, 599, 280, 930, 121, 46]\n",
      "The best and most reliable form of research is the double-blind, placebo-controlled study.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'The best and most reliable form of research is the double-blind, placebo-controlled study.'\n",
    "print(tokenizer.display(prompt))\n",
    "\n",
    "tokens = tokenizer.encode(prompt)\n",
    "print(tokens)\n",
    "\n",
    "decoded = tokenizer.decode(tokens)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ba18a30-c97c-41bc-a37b-23e82bd7d055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: '\u0000'\n",
      "1: '\u0001'\n",
      "2: '\u0002'\n",
      "3: '\u0003'\n",
      "4: '\u0004'\n",
      "5: '\u0005'\n",
      "6: '\u0006'\n",
      "7: '\u0007'\n",
      "8: '\n",
      "9: '\t'\n",
      "10: '\n",
      "'\n",
      "11: '\u000b",
      "'\n",
      "12: '\f",
      "'\n",
      "'3: '\n",
      "14: '\u000e'\n",
      "15: '\u000f'\n",
      "16: '\u0010'\n",
      "17: '\u0011'\n",
      "18: '\u0012'\n",
      "19: '\u0013'\n",
      "20: '\u0014'\n",
      "21: '\u0015'\n",
      "22: '\u0016'\n",
      "23: '\u0017'\n",
      "24: '\u0018'\n",
      "25: '\u0019'\n",
      "26: '\u001a'\n",
      "27: '\u001b'\n",
      "28: '\u001c",
      "'\n",
      "29: '\u001d",
      "'\n",
      "30: '\u001e",
      "'\n",
      "31: '\u001f'\n",
      "32: ' '\n",
      "33: '!'\n",
      "34: '\"'\n",
      "35: '#'\n",
      "36: '$'\n",
      "37: '%'\n",
      "38: '&'\n",
      "39: '''\n",
      "40: '('\n",
      "41: ')'\n",
      "42: '*'\n",
      "43: '+'\n",
      "44: ','\n",
      "45: '-'\n",
      "46: '.'\n",
      "47: '/'\n",
      "48: '0'\n",
      "49: '1'\n",
      "50: '2'\n",
      "51: '3'\n",
      "52: '4'\n",
      "53: '5'\n",
      "54: '6'\n",
      "55: '7'\n",
      "56: '8'\n",
      "57: '9'\n",
      "58: ':'\n",
      "59: ';'\n",
      "60: '<'\n",
      "61: '='\n",
      "62: '>'\n",
      "63: '?'\n",
      "64: '@'\n",
      "65: 'A'\n",
      "66: 'B'\n",
      "67: 'C'\n",
      "68: 'D'\n",
      "69: 'E'\n",
      "70: 'F'\n",
      "71: 'G'\n",
      "72: 'H'\n",
      "73: 'I'\n",
      "74: 'J'\n",
      "75: 'K'\n",
      "76: 'L'\n",
      "77: 'M'\n",
      "78: 'N'\n",
      "79: 'O'\n",
      "80: 'P'\n",
      "81: 'Q'\n",
      "82: 'R'\n",
      "83: 'S'\n",
      "84: 'T'\n",
      "85: 'U'\n",
      "86: 'V'\n",
      "87: 'W'\n",
      "88: 'X'\n",
      "89: 'Y'\n",
      "90: 'Z'\n",
      "91: '['\n",
      "92: '\\'\n",
      "93: ']'\n",
      "94: '^'\n",
      "95: '_'\n",
      "96: '`'\n",
      "97: 'a'\n",
      "98: 'b'\n",
      "99: 'c'\n",
      "100: 'd'\n",
      "101: 'e'\n",
      "102: 'f'\n",
      "103: 'g'\n",
      "104: 'h'\n",
      "105: 'i'\n",
      "106: 'j'\n",
      "107: 'k'\n",
      "108: 'l'\n",
      "109: 'm'\n",
      "110: 'n'\n",
      "111: 'o'\n",
      "112: 'p'\n",
      "113: 'q'\n",
      "114: 'r'\n",
      "115: 's'\n",
      "116: 't'\n",
      "117: 'u'\n",
      "118: 'v'\n",
      "119: 'w'\n",
      "120: 'x'\n",
      "121: 'y'\n",
      "122: 'z'\n",
      "123: '{'\n",
      "124: '|'\n",
      "125: '}'\n",
      "126: '~'\n",
      "127: ''\n",
      "128: '�'\n",
      "129: '�'\n",
      "130: '�'\n",
      "131: '�'\n",
      "132: '�'\n",
      "133: '�'\n",
      "134: '�'\n",
      "135: '�'\n",
      "136: '�'\n",
      "137: '�'\n",
      "138: '�'\n",
      "139: '�'\n",
      "140: '�'\n",
      "141: '�'\n",
      "142: '�'\n",
      "143: '�'\n",
      "144: '�'\n",
      "145: '�'\n",
      "146: '�'\n",
      "147: '�'\n",
      "148: '�'\n",
      "149: '�'\n",
      "150: '�'\n",
      "151: '�'\n",
      "152: '�'\n",
      "153: '�'\n",
      "154: '�'\n",
      "155: '�'\n",
      "156: '�'\n",
      "157: '�'\n",
      "158: '�'\n",
      "159: '�'\n",
      "160: '�'\n",
      "161: '�'\n",
      "162: '�'\n",
      "163: '�'\n",
      "164: '�'\n",
      "165: '�'\n",
      "166: '�'\n",
      "167: '�'\n",
      "168: '�'\n",
      "169: '�'\n",
      "170: '�'\n",
      "171: '�'\n",
      "172: '�'\n",
      "173: '�'\n",
      "174: '�'\n",
      "175: '�'\n",
      "176: '�'\n",
      "177: '�'\n",
      "178: '�'\n",
      "179: '�'\n",
      "180: '�'\n",
      "181: '�'\n",
      "182: '�'\n",
      "183: '�'\n",
      "184: '�'\n",
      "185: '�'\n",
      "186: '�'\n",
      "187: '�'\n",
      "188: '�'\n",
      "189: '�'\n",
      "190: '�'\n",
      "191: '�'\n",
      "192: '�'\n",
      "193: '�'\n",
      "194: '�'\n",
      "195: '�'\n",
      "196: '�'\n",
      "197: '�'\n",
      "198: '�'\n",
      "199: '�'\n",
      "200: '�'\n",
      "201: '�'\n",
      "202: '�'\n",
      "203: '�'\n",
      "204: '�'\n",
      "205: '�'\n",
      "206: '�'\n",
      "207: '�'\n",
      "208: '�'\n",
      "209: '�'\n",
      "210: '�'\n",
      "211: '�'\n",
      "212: '�'\n",
      "213: '�'\n",
      "214: '�'\n",
      "215: '�'\n",
      "216: '�'\n",
      "217: '�'\n",
      "218: '�'\n",
      "219: '�'\n",
      "220: '�'\n",
      "221: '�'\n",
      "222: '�'\n",
      "223: '�'\n",
      "224: '�'\n",
      "225: '�'\n",
      "226: '�'\n",
      "227: '�'\n",
      "228: '�'\n",
      "229: '�'\n",
      "230: '�'\n",
      "231: '�'\n",
      "232: '�'\n",
      "233: '�'\n",
      "234: '�'\n",
      "235: '�'\n",
      "236: '�'\n",
      "237: '�'\n",
      "238: '�'\n",
      "239: '�'\n",
      "240: '�'\n",
      "241: '�'\n",
      "242: '�'\n",
      "243: '�'\n",
      "244: '�'\n",
      "245: '�'\n",
      "246: '�'\n",
      "247: '�'\n",
      "248: '�'\n",
      "249: '�'\n",
      "250: '�'\n",
      "251: '�'\n",
      "252: '�'\n",
      "253: '�'\n",
      "254: '�'\n",
      "255: '�'\n",
      "256: ' t'\n",
      "257: ' a'\n",
      "258: 'in'\n",
      "259: 'he'\n",
      "260: 're'\n",
      "261: 'on'\n",
      "262: 'er'\n",
      "263: ' the'\n",
      "264: ' s'\n",
      "265: ' w'\n",
      "266: ' o'\n",
      "267: 'at'\n",
      "268: 'nd'\n",
      "269: ' c'\n",
      "270: 'es'\n",
      "271: 'or'\n",
      "272: 'it'\n",
      "273: 'en'\n",
      "274: 'is'\n",
      "275: ' f'\n",
      "276: 'ing'\n",
      "277: ' b'\n",
      "278: 'ou'\n",
      "279: ' p'\n",
      "280: 'ed'\n",
      "281: 'an'\n",
      "282: 'al'\n",
      "283: 'ar'\n",
      "284: ' to'\n",
      "285: ' m'\n",
      "286: ' in'\n",
      "287: ' of'\n",
      "288: ' and'\n",
      "289: 'le'\n",
      "290: 'ic'\n",
      "291: 'as'\n",
      "292: ' h'\n",
      "293: ' d'\n",
      "294: 'om'\n",
      "295: 'ion'\n",
      "296: ' th'\n",
      "297: 'il'\n",
      "298: 'ent'\n",
      "299: 'st'\n",
      "300: '.\n",
      "'\n",
      "301: 've'\n",
      "302: 'ro'\n",
      "303: ' re'\n",
      "304: ' l'\n",
      "305: ' e'\n",
      "306: ' S'\n",
      "307: ' n'\n",
      "308: ' g'\n",
      "309: 'et'\n",
      "310: 'ac'\n",
      "311: ' T'\n",
      "312: ' A'\n",
      "313: 'ay'\n",
      "314: ' I'\n",
      "315: ' C'\n",
      "316: 'id'\n",
      "317: '�'\n",
      "318: 'ly'\n",
      "319: 'ut'\n",
      "320: ' for'\n",
      "321: ' be'\n",
      "322: ' on'\n",
      "323: 'ot'\n",
      "324: ' y'\n",
      "325: 'am'\n",
      "326: ' is'\n",
      "327: 'ol'\n",
      "328: 'ad'\n",
      "329: 'ur'\n",
      "330: 'ow'\n",
      "331: 'ver'\n",
      "332: 'im'\n",
      "333: 'us'\n",
      "334: 'ig'\n",
      "335: ' M'\n",
      "336: 'ct'\n",
      "337: 'el'\n",
      "338: ' B'\n",
      "339: 'ch'\n",
      "340: ' that'\n",
      "341: '20'\n",
      "342: 'ation'\n",
      "343: 'oo'\n",
      "344: ' P'\n",
      "345: ' st'\n",
      "346: 'ith'\n",
      "347: ' you'\n",
      "348: 'ir'\n",
      "349: 'ce'\n",
      "350: ' it'\n",
      "351: ' ('\n",
      "352: ' with'\n",
      "353: 'ill'\n",
      "354: 'un'\n",
      "355: ' W'\n",
      "356: 'ter'\n",
      "357: ' we'\n",
      "358: ' he'\n",
      "359: ' R'\n",
      "360: ' F'\n",
      "361: ' as'\n",
      "362: 'se'\n",
      "363: ' al'\n",
      "364: ' G'\n",
      "365: ' wh'\n",
      "366: 'if'\n",
      "367: 'ul'\n",
      "368: ' an'\n",
      "369: 'ers'\n",
      "370: ' ha'\n",
      "371: '201'\n",
      "372: 'ke'\n",
      "373: ' at'\n",
      "374: 'em'\n",
      "375: 'ess'\n",
      "376: ' H'\n",
      "377: 'th'\n",
      "378: ' pro'\n",
      "379: ' con'\n",
      "380: '’'\n",
      "381: ' r'\n",
      "382: 'her'\n",
      "383: 'ag'\n",
      "384: ' D'\n",
      "385: ' are'\n",
      "386: 'ate'\n",
      "387: ' com'\n",
      "388: 'pp'\n",
      "389: 'ew'\n",
      "390: 'ld'\n",
      "391: ' The'\n",
      "392: ''s'\n",
      "393: ' or'\n",
      "394: 'est'\n",
      "395: ' fr'\n",
      "396: 'ore'\n",
      "397: ' E'\n",
      "398: ' N'\n",
      "399: 'igh'\n",
      "400: 'ain'\n",
      "401: 'ist'\n",
      "402: ' se'\n",
      "403: 'pe'\n",
      "404: ' was'\n",
      "405: ' -'\n",
      "406: 'and'\n",
      "407: ' L'\n",
      "408: 'ment'\n",
      "409: ' O'\n",
      "410: 'ive'\n",
      "411: 'ies'\n",
      "412: 'ts'\n",
      "413: 'ab'\n",
      "414: 'ure'\n",
      "415: ' have'\n",
      "416: ' ex'\n",
      "417: 'ear'\n",
      "418: ' from'\n",
      "419: 'um'\n",
      "420: 'ust'\n",
      "421: 'ity'\n",
      "422: ' sh'\n",
      "423: 'all'\n",
      "424: 'our'\n",
      "425: ' this'\n",
      "426: 'up'\n",
      "427: 'ri'\n",
      "428: 'oc'\n",
      "429: ' J'\n",
      "430: 'qu'\n",
      "431: 'op'\n",
      "432: ' v'\n",
      "433: ' de'\n",
      "434: 'ast'\n",
      "435: 'ost'\n",
      "436: 'art'\n",
      "437: ' us'\n",
      "438: ' by'\n",
      "439: 'os'\n",
      "440: 'ap'\n",
      "441: ' not'\n",
      "442: 'The'\n",
      "443: 'ould'\n",
      "444: ' will'\n",
      "445: 'res'\n",
      "446: 'ight'\n",
      "447: 'rou'\n",
      "448: ' le'\n",
      "449: 'ort'\n",
      "450: 'ard'\n",
      "451: 'ome'\n",
      "452: 'ud'\n",
      "453: ' j'\n",
      "454: 'ook'\n",
      "455: 'iv'\n",
      "456: 'ace'\n",
      "457: 'ack'\n",
      "458: ' �'\n",
      "459: ' can'\n",
      "460: ' U'\n",
      "461: 'nt'\n",
      "462: 'ial'\n",
      "463: 'ak'\n",
      "464: ' ch'\n",
      "465: '’s'\n",
      "466: 'are'\n",
      "467: ' sa'\n",
      "468: 'ine'\n",
      "469: ' pl'\n",
      "470: ' has'\n",
      "471: ' tr'\n",
      "472: 'ie'\n",
      "473: 'te'\n",
      "474: ' do'\n",
      "475: 'gh'\n",
      "476: ' ab'\n",
      "477: 'ell'\n",
      "478: 'one'\n",
      "479: ' your'\n",
      "480: 'ant'\n",
      "481: 'ge'\n",
      "482: ' su'\n",
      "483: 'out'\n",
      "484: 'ff'\n",
      "485: 'ice'\n",
      "486: ' K'\n",
      "487: 'ip'\n",
      "488: 'od'\n",
      "489: ' all'\n",
      "490: 'ood'\n",
      "491: ' wor'\n",
      "492: 'ide'\n",
      "493: 'ally'\n",
      "494: ' comp'\n",
      "495: 'ue'\n",
      "496: ' but'\n",
      "497: ' out'\n",
      "498: ' up'\n",
      "499: 'ber'\n",
      "500: ' ad'\n",
      "501: 'per'\n",
      "502: ' ne'\n",
      "503: ' whe'\n",
      "504: ' our'\n",
      "505: ' me'\n",
      "506: ' k'\n",
      "507: ' \"'\n",
      "508: 'og'\n",
      "509: 'ord'\n",
      "510: ' te'\n",
      "511: 'ions'\n",
      "512: ' In'\n",
      "513: 'ime'\n",
      "514: ' their'\n",
      "515: 'ry'\n",
      "516: 'red'\n",
      "517: ' en'\n",
      "518: 'ame'\n",
      "519: ' cl'\n",
      "520: ' Th'\n",
      "521: ' more'\n",
      "522: '00'\n",
      "523: 'ind'\n",
      "524: 'ong'\n",
      "525: 'ark'\n",
      "526: ' Ch'\n",
      "527: ' inc'\n",
      "528: '||'\n",
      "529: 'ous'\n",
      "530: ' off'\n",
      "531: ' who'\n",
      "532: 'ich'\n",
      "533: 'ake'\n",
      "534: ' St'\n",
      "535: ' they'\n",
      "536: 'ther'\n",
      "537: ' one'\n",
      "538: 'ated'\n",
      "539: 'pt'\n",
      "540: 'ail'\n",
      "541: 'cc'\n",
      "542: 'very'\n",
      "543: 'ven'\n",
      "544: 'ance'\n",
      "545: '”'\n",
      "546: ' said'\n",
      "547: 'ick'\n",
      "548: ' li'\n",
      "549: ' un'\n",
      "550: 'orm'\n",
      "551: 'act'\n",
      "552: ' per'\n",
      "553: 'iz'\n",
      "554: ' man'\n",
      "555: 'du'\n",
      "556: 'age'\n",
      "557: ' so'\n",
      "558: 'ans'\n",
      "559: '..'\n",
      "560: 'ia'\n",
      "561: 'ck'\n",
      "562: ' cont'\n",
      "563: ' sp'\n",
      "564: 'erv'\n",
      "565: 'und'\n",
      "566: ' were'\n",
      "567: '200'\n",
      "568: 'ub'\n",
      "569: 'ations'\n",
      "570: 'able'\n",
      "571: ' new'\n",
      "572: 'ile'\n",
      "573: 'ect'\n",
      "574: ' about'\n",
      "575: 'ire'\n",
      "576: 'ra'\n",
      "577: 'ove'\n",
      "578: 'ign'\n",
      "579: ' year'\n",
      "580: 'ary'\n",
      "581: ' his'\n",
      "582: ' V'\n",
      "583: ' over'\n",
      "584: ' rec'\n",
      "585: 'pl'\n",
      "586: 'so'\n",
      "587: 'ob'\n",
      "588: 'ite'\n",
      "589: ' “'\n",
      "590: ' Y'\n",
      "591: ' go'\n",
      "592: ''t'\n",
      "593: 'reat'\n",
      "594: 'ction'\n",
      "595: 'ical'\n",
      "596: 'ase'\n",
      "597: 'ach'\n",
      "598: 'av'\n",
      "599: 'll'\n",
      "600: ' im'\n",
      "601: ' res'\n",
      "602: ' time'\n",
      "603: 'ough'\n",
      "604: ' its'\n",
      "605: ' Ad'\n",
      "606: ' there'\n",
      "607: ' We'\n",
      "608: ' ag'\n",
      "609: ' would'\n",
      "610: 'vel'\n",
      "611: 'form'\n",
      "612: ' like'\n",
      "613: ' Re'\n",
      "614: 'gle'\n",
      "615: 'ere'\n",
      "616: 'ass'\n",
      "617: 'ition'\n",
      "618: 'to'\n",
      "619: 'ink'\n",
      "620: 'ens'\n",
      "621: 'ib'\n",
      "622: ' pr'\n",
      "623: ' get'\n",
      "624: ' work'\n",
      "625: ' other'\n",
      "626: 'che'\n",
      "627: ' been'\n",
      "628: ' qu'\n",
      "629: 'amp'\n",
      "630: 'ence'\n",
      "631: 'wn'\n",
      "632: 'ov'\n",
      "633: 'oogle'\n",
      "634: 'ish'\n",
      "635: 'now'\n",
      "636: 'ose'\n",
      "637: 'ount'\n",
      "638: 'ree'\n",
      "639: 'day'\n",
      "640: 'ru'\n",
      "641: 'com'\n",
      "642: ' Google'\n",
      "643: ' It'\n",
      "644: 'mer'\n",
      "645: ' also'\n",
      "646: ' some'\n",
      "647: 'ors'\n",
      "648: ' which'\n",
      "649: 'oy'\n",
      "650: 'ings'\n",
      "651: ' just'\n",
      "652: 'old'\n",
      "653: '19'\n",
      "654: ' them'\n",
      "655: ' spe'\n",
      "656: ' had'\n",
      "657: 'ater'\n",
      "658: ' if'\n",
      "659: ' pe'\n",
      "660: ' than'\n",
      "661: ' fir'\n",
      "662: 'ents'\n",
      "663: 'vers'\n",
      "664: ' tw'\n",
      "665: ' my'\n",
      "666: 'ress'\n",
      "667: 'ting'\n",
      "668: 'ons'\n",
      "669: ' when'\n",
      "670: ' any'\n",
      "671: 'ates'\n",
      "672: ' app'\n",
      "673: 'lud'\n",
      "674: 'ond'\n",
      "675: ' pre'\n",
      "676: ' what'\n",
      "677: 'low'\n",
      "678: ' dis'\n",
      "679: 'ory'\n",
      "680: 'ng'\n",
      "681: 'ays'\n",
      "682: 'book'\n",
      "683: 'iness'\n",
      "684: 'ade'\n",
      "685: 'anc'\n",
      "686: 'ne'\n",
      "687: 'oin'\n",
      "688: 'aus'\n",
      "689: ' bu'\n",
      "690: 'eth'\n",
      "691: ' sc'\n",
      "692: 'acebook'\n",
      "693: ' fe'\n",
      "694: 'urn'\n",
      "695: 'fter'\n",
      "696: 'fere'\n",
      "697: '?\n",
      "'\n",
      "698: ' acc'\n",
      "699: ' ro'\n",
      "700: 'ek'\n",
      "701: ' part'\n",
      "702: ' first'\n",
      "703: ' $'\n",
      "704: ' need'\n",
      "705: '.\n",
      "\n",
      "'\n",
      "706: ' ar'\n",
      "707: ' cons'\n",
      "708: 'ang'\n",
      "709: 'ild'\n",
      "710: 'ool'\n",
      "711: ' fin'\n",
      "712: ' imp'\n",
      "713: ' Facebook'\n",
      "714: 'ople'\n",
      "715: 'lic'\n",
      "716: ' two'\n",
      "717: ' fl'\n",
      "718: ' bl'\n",
      "719: ' am'\n",
      "720: 'ss'\n",
      "721: 'ult'\n",
      "722: 'usiness'\n",
      "723: 'arket'\n",
      "724: ' she'\n",
      "725: 'ments'\n",
      "726: ' bec'\n",
      "727: ' know'\n",
      "728: 'iss'\n",
      "729: ' includ'\n",
      "730: ' Com'\n",
      "731: ' into'\n",
      "732: 'under'\n",
      "733: 'ep'\n",
      "734: 'ian'\n",
      "735: ' add'\n",
      "736: '15'\n",
      "737: ' serv'\n",
      "738: 'ious'\n",
      "739: 'arch'\n",
      "740: 'May'\n",
      "741: 'tern'\n",
      "742: '’t'\n",
      "743: ' no'\n",
      "744: '10'\n",
      "745: 'any'\n",
      "746: 'round'\n",
      "747: 'rough'\n",
      "748: ' comm'\n",
      "749: 'ning'\n",
      "750: 'ise'\n",
      "751: 'roup'\n",
      "752: 'eb'\n",
      "753: ' Wh'\n",
      "754: ' only'\n",
      "755: ' New'\n",
      "756: 'rit'\n",
      "757: 'own'\n",
      "758: ' how'\n",
      "759: ' see'\n",
      "760: 'ices'\n",
      "761: ' bet'\n",
      "762: 'ft'\n",
      "763: ' under'\n",
      "764: ' her'\n",
      "765: 'ock'\n",
      "766: ' back'\n",
      "767: ' team'\n",
      "768: ' pol'\n",
      "769: ' play'\n",
      "770: 'Th'\n",
      "771: ' ind'\n",
      "772: ' sy'\n",
      "773: 'ank'\n",
      "774: 'ict'\n",
      "775: '|\n",
      "'\n",
      "776: ' good'\n",
      "777: 'ific'\n",
      "778: 'ities'\n",
      "779: ' people'\n",
      "780: 'row'\n",
      "781: 'get'\n",
      "782: ' rep'\n",
      "783: 'ont'\n",
      "784: ' col'\n",
      "785: ' well'\n",
      "786: 'oun'\n",
      "787: 'earch'\n",
      "788: 'ound'\n",
      "789: ' after'\n",
      "790: ' most'\n",
      "791: 'ual'\n",
      "792: 'ause'\n",
      "793: 'hed'\n",
      "794: 'gr'\n",
      "795: ' these'\n",
      "796: ' make'\n",
      "797: ' even'\n",
      "798: '11'\n",
      "799: 'ature'\n",
      "800: ' last'\n",
      "801: 'ful'\n",
      "802: 'ix'\n",
      "803: ' every'\n",
      "804: 'fore'\n",
      "805: ' An'\n",
      "806: ' des'\n",
      "807: 'ues'\n",
      "808: 'ble'\n",
      "809: ' This'\n",
      "810: ' where'\n",
      "811: 'ty'\n",
      "812: 'aw'\n",
      "813: 'iew'\n",
      "814: ' could'\n",
      "815: 'les'\n",
      "816: ' em'\n",
      "817: 'ied'\n",
      "818: ' Pro'\n",
      "819: ' prov'\n",
      "820: ' che'\n",
      "821: ' supp'\n",
      "822: ' produ'\n",
      "823: ',\"'\n",
      "824: 'oth'\n",
      "825: 'pr'\n",
      "826: 'ational'\n",
      "827: 'ug'\n",
      "828: 'az'\n",
      "829: 'ward'\n",
      "830: ' win'\n",
      "831: ' look'\n",
      "832: 'rist'\n",
      "833: ' inv'\n",
      "834: 'lect'\n",
      "835: ' ass'\n",
      "836: ' Se'\n",
      "837: 'ull'\n",
      "838: ' again'\n",
      "839: 'ert'\n",
      "840: 'tle'\n",
      "841: 'we'\n",
      "842: '.\"'\n",
      "843: ' may'\n",
      "844: ' reg'\n",
      "845: 'ork'\n",
      "846: ' through'\n",
      "847: 'uch'\n",
      "848: ' think'\n",
      "849: 'cial'\n",
      "850: 'ating'\n",
      "851: 'atch'\n",
      "852: 'ily'\n",
      "853: 'hone'\n",
      "854: '.com'\n",
      "855: 'als'\n",
      "856: 'ason'\n",
      "857: ' pos'\n",
      "858: 'uro'\n",
      "859: ' use'\n",
      "860: 'man'\n",
      "861: ' did'\n",
      "862: 'ces'\n",
      "863: ' He'\n",
      "864: 'alth'\n",
      "865: 'land'\n",
      "866: 'ased'\n",
      "867: 'ness'\n",
      "868: ' great'\n",
      "869: 'air'\n",
      "870: 'pen'\n",
      "871: 'illion'\n",
      "872: ' act'\n",
      "873: 'ax'\n",
      "874: ' gu'\n",
      "875: 'aking'\n",
      "876: 'rap'\n",
      "877: 'rib'\n",
      "878: 'ars'\n",
      "879: 'ream'\n",
      "880: 'In'\n",
      "881: 'alk'\n",
      "882: ' rem'\n",
      "883: ' att'\n",
      "884: ' br'\n",
      "885: ' way'\n",
      "886: 'vent'\n",
      "887: 'ative'\n",
      "888: 'olog'\n",
      "889: ' now'\n",
      "890: '\n",
      "\n",
      "'\n",
      "891: '16'\n",
      "892: 'nds'\n",
      "893: 'ience'\n",
      "894: 'ys'\n",
      "895: ' week'\n",
      "896: '.”'\n",
      "897: ' own'\n",
      "898: ' made'\n",
      "899: ' dec'\n",
      "900: 'way'\n",
      "901: ' loc'\n",
      "902: 'uring'\n",
      "903: 'ility'\n",
      "904: 'ident'\n",
      "905: ' years'\n",
      "906: ' ac'\n",
      "907: ' here'\n",
      "908: ' som'\n",
      "909: 'by'\n",
      "910: ' –'\n",
      "911: 'ement'\n",
      "912: 'ublic'\n",
      "913: 'ets'\n",
      "914: ' many'\n",
      "915: ')\n",
      "'\n",
      "916: ' And'\n",
      "917: 'ife'\n",
      "918: 'ics'\n",
      "919: 'hip'\n",
      "920: 'We'\n",
      "921: 'xt'\n",
      "922: ' min'\n",
      "923: 'ving'\n",
      "924: ' Cl'\n",
      "925: ' sec'\n",
      "926: 'ems'\n",
      "927: ' don'\n",
      "928: 'pect'\n",
      "929: 'ering'\n",
      "930: ' stud'\n",
      "931: 'ject'\n",
      "932: ' pres'\n",
      "933: 'ced'\n",
      "934: 'ters'\n",
      "935: 'read'\n",
      "936: ' very'\n",
      "937: ' Ar'\n",
      "938: ' really'\n",
      "939: 'ins'\n",
      "940: ' ret'\n",
      "941: ' day'\n",
      "942: ' long'\n",
      "943: ' De'\n",
      "944: 'ms'\n",
      "945: 'its'\n",
      "946: ' You'\n",
      "947: 'ivers'\n",
      "948: ' hel'\n",
      "949: ' business'\n",
      "950: ' because'\n",
      "951: ' lead'\n",
      "952: 'chool'\n",
      "953: 'ourn'\n",
      "954: ':\n",
      "'\n",
      "955: 'velop'\n",
      "956: 'oh'\n",
      "957: ' exper'\n",
      "958: 'ash'\n",
      "959: ' mem'\n",
      "960: ' being'\n",
      "961: ' cr'\n",
      "962: 'ave'\n",
      "963: ' game'\n",
      "964: 'ted'\n",
      "965: ' Un'\n",
      "966: 'ange'\n",
      "967: 'ner'\n",
      "968: ' cap'\n",
      "969: 'nce'\n",
      "970: ' help'\n",
      "971: 'ited'\n",
      "972: ' too'\n",
      "973: ' cle'\n",
      "974: 'meric'\n",
      "975: 'uck'\n",
      "976: ' Euro'\n",
      "977: ' Sh'\n",
      "978: 'erson'\n",
      "979: ' start'\n",
      "980: ' such'\n",
      "981: ' Bl'\n",
      "982: ' lit'\n",
      "983: ' him'\n",
      "984: ' dif'\n",
      "985: ' Christ'\n",
      "986: 'outh'\n",
      "987: 'ible'\n",
      "988: 'other'\n",
      "989: ' then'\n",
      "990: ' bo'\n",
      "991: ' count'\n",
      "992: 'hen'\n",
      "993: 'ah'\n",
      "994: ' end'\n",
      "995: 'end'\n",
      "996: 'cent'\n",
      "997: ' does'\n",
      "998: 'elf'\n",
      "999: ' mon'\n",
      "1000: 'bs'\n",
      "1001: ' ent'\n",
      "1002: 'ph'\n",
      "1003: ' Be'\n",
      "1004: ' ins'\n",
      "1005: 'ular'\n",
      "1006: ' For'\n",
      "1007: 'ird'\n",
      "1008: '...'\n",
      "1009: 'imes'\n",
      "1010: ' poin'\n",
      "1011: ' show'\n",
      "1012: 'ually'\n",
      "1013: ' i'\n",
      "1014: ' say'\n",
      "1015: 'ene'\n",
      "1016: 'aim'\n",
      "1017: ' co'\n",
      "1018: ' sm'\n",
      "1019: ' much'\n",
      "1020: ' ear'\n"
     ]
    }
   ],
   "source": [
    "for i in range(vocab_size-3):\n",
    "    print(f\"{i}: '{tokenizer.decode([i])}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8967b6-f78b-4852-90f3-f3a1f37455cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
