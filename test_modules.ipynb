{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea126987-59aa-4f76-b926-6d632887c30b",
   "metadata": {},
   "source": [
    "# This notebook is designed for teaching/testing purposes to help you visualize the tensor shapes that go through each module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f833b1f8-ea91-4ae5-b3a3-73e08e4c8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that. \n",
    "# you prolly won't need this cell but running it won't hurt anything either\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72c2c04f-2dbd-4020-8d91-cc0e4e8511b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelConfig(dim=8, device='mps', linear_bias=False, out_weight_share=True, max_seq_len=16, pos_enc_type='RoPE', theta=10000, tokenizer='bpe_tinyStories', vocab_len=2048, num_layers=1, second_resid_norm=False, mlp_hidden_mult=4, mlp_nonlinearity='SiLU', mlp_gated=True, num_q_heads=2, num_kv_heads=1, head_dim=4, scale_first_resid=True, norm_type='RMSNorm', norm_affine=True, norm_bias=True, eps=1e-06)\n",
      "TrainConfig(model_name='2024-09-16|18-33-04', dataset_name='noanabeshima/TinyStoriesV2', data_subset=None, streaming=True, micro_batch_size=2, grad_accum_steps=2, max_iters=50, eval_interval=5, eval_samples=1, checkpoint_interval=None, beta1=0.9, beta2=0.95, epsilon=1e-08, weight_decay=0.05, grad_clip=1.0, lr_init=1e-06, lr_max=0.01, lr_min=0.0001, warmup_iters=5, final_flat_iters=2, anneal_type='cos', num_restarts=0, T_mult=2)\n"
     ]
    }
   ],
   "source": [
    "# config file\n",
    "from config import ModelConfig, TrainConfig\n",
    "cfg = ModelConfig()\n",
    "tcfg = TrainConfig()\n",
    "print(cfg)\n",
    "print(tcfg)\n",
    "\n",
    "# import the tokenizer specified by cfg\n",
    "from tools import import_from_nested_path\n",
    "imported_objects = import_from_nested_path(['custom_tokenizers', cfg.tokenizer], 'tokenizer', ['get_tokenizer'])\n",
    "get_tokenizer = imported_objects.get('get_tokenizer')\n",
    "tokenizer = get_tokenizer(size = 512) # assuming 'bpe_tinyStories', size options are 512, 1024 and 2048\n",
    "\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c12e3b-dc63-4479-ad55-b05d96364d1f",
   "metadata": {},
   "source": [
    "# Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3debc7b1-a7ec-4fb3-98cb-d16edf7c71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.norm import Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "627969e9-9017-43f3-90ec-9a485abef26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.02K\n",
      "Norm()\n",
      "\n",
      "====================Entering Norm.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -2.868/2.869\n",
      "\n",
      "====================Entering Norm.RMSNorm====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -2.868/2.869\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -2.352/2.371\n",
      "====================Exiting Norm.RMSNorm====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -2.352/2.371\n",
      "====================Exiting Norm.forward====================\n",
      "CPU times: user 49.1 ms, sys: 41 ms, total: 90.2 ms\n",
      "Wall time: 117 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### RMSNorm\n",
    "\n",
    "# Create an instance of RMSNorm\n",
    "module = Norm(cfg.dim, 'RMSNorm').to(cfg.device)\n",
    "\n",
    "# let's take a look\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "\n",
    "# Initially, logging is disabled by default\n",
    "module.enable_logging()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('CosineNorm')\n",
    "#module.disable_function_logging('LayerNorm')\n",
    "#module.disable_function_logging('RMSNorm')\n",
    "\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "output = module(x)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64020a96-205a-45b7-ae56-a3555d2b3719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================Entering Norm.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -2.426/2.874\n",
      "Full tensor content:\n",
      "tensor([[[-1.0500, -0.1534, -0.4999,  0.6605, -0.2287,  1.7829, -0.6294,\n",
      "           0.1742],\n",
      "         [ 0.2729, -0.4710,  0.5910, -0.7012,  0.1361,  1.4918,  0.4335,\n",
      "          -1.0387],\n",
      "         [ 0.2086, -0.5668, -0.1605,  0.2868, -2.2282, -0.5542,  0.7998,\n",
      "          -1.2815],\n",
      "         [ 0.7306,  0.0831, -0.8461,  0.5909, -0.3932,  0.3018,  0.5686,\n",
      "           0.6200],\n",
      "         [-0.2238, -0.7060,  1.8229,  0.7945, -0.7008, -0.6020, -1.5264,\n",
      "           0.0326],\n",
      "         [ 2.8740, -0.0607,  1.1707, -0.3802, -0.5758, -1.7617,  0.4685,\n",
      "          -1.4737],\n",
      "         [ 0.5714,  0.9737,  0.9463,  0.8130,  0.3562,  0.7989,  0.6797,\n",
      "          -0.4417],\n",
      "         [-0.2639, -0.4763,  0.6674,  0.1654,  1.3941,  0.4235,  1.8382,\n",
      "          -1.7479],\n",
      "         [-1.2054,  0.2419,  0.3639,  0.5018, -1.1669,  0.8820,  0.5607,\n",
      "          -0.3508],\n",
      "         [ 0.8573, -0.7545,  0.3449, -1.1696, -0.1534,  0.6343, -1.2354,\n",
      "          -1.1734],\n",
      "         [-0.4871, -0.1617,  0.3188, -0.3874,  1.2219,  0.1743,  0.4443,\n",
      "           0.9441],\n",
      "         [-0.4933, -0.9870,  1.2818, -1.3832,  0.1202, -2.1615, -1.0478,\n",
      "           0.4527],\n",
      "         [-0.1114, -0.1225,  1.7615, -0.7113, -0.6273,  0.4197, -0.7589,\n",
      "           0.5602],\n",
      "         [ 0.2764,  0.6196,  0.8415, -0.7104,  0.0316,  0.6711,  2.0246,\n",
      "          -0.0484],\n",
      "         [ 0.7228,  0.2619, -0.3030,  0.0460,  0.9620, -1.0834,  2.1172,\n",
      "           0.6024],\n",
      "         [ 0.7484,  1.0675, -0.6769,  0.0952,  1.2399, -0.5372, -0.2298,\n",
      "           2.2404]],\n",
      "\n",
      "        [[-1.4238, -0.6464, -0.3003, -0.3567, -0.1210,  1.1321, -0.6748,\n",
      "          -0.6645],\n",
      "         [ 0.6674,  1.8005,  2.4290, -0.8607, -0.4747, -0.1016,  2.0740,\n",
      "          -0.2857],\n",
      "         [-1.4213,  1.1788,  0.1805, -0.7369,  0.1761,  0.1399,  0.0629,\n",
      "          -2.0777],\n",
      "         [-0.7911,  0.9655, -1.4965,  1.1998,  0.9893, -1.3574,  0.5672,\n",
      "           1.2281],\n",
      "         [ 0.9243,  0.5867, -0.3333, -1.9671,  0.2039, -1.2445,  1.2913,\n",
      "           0.6352],\n",
      "         [ 0.4935,  0.5510, -0.3564, -0.0378,  0.2247, -1.0182, -0.0616,\n",
      "          -0.1109],\n",
      "         [-0.0190, -0.9928, -0.4837,  1.1592,  1.4838, -0.5304,  1.0039,\n",
      "          -0.4426],\n",
      "         [ 1.2183,  1.7060, -0.5656, -0.9355, -1.5974, -1.7481, -0.7165,\n",
      "           0.1227],\n",
      "         [ 0.0229, -0.4363, -0.8647,  0.2886,  1.9674,  1.3417,  1.7155,\n",
      "          -0.0825],\n",
      "         [-2.4260, -2.1571, -0.2377,  0.4924, -0.4597,  0.0410, -0.6809,\n",
      "           0.5661],\n",
      "         [-1.0427,  1.7310,  0.0256, -0.4507, -0.3847,  0.0815,  0.1870,\n",
      "           0.8699],\n",
      "         [ 0.0745, -0.5198, -0.3924,  1.1076,  0.0591, -0.3113,  0.0696,\n",
      "           0.0575],\n",
      "         [-0.4326,  1.3485,  0.0088, -0.7957,  0.5488, -0.1240,  1.1093,\n",
      "           0.2768],\n",
      "         [ 0.7184,  0.8009, -0.6520, -0.5444,  1.3001,  1.3125, -0.4642,\n",
      "           1.3442],\n",
      "         [ 1.2597, -0.0219,  0.3184,  0.5466, -0.2180, -0.3436,  0.1015,\n",
      "          -0.9529],\n",
      "         [ 1.0704,  0.6365,  1.0409,  0.3957,  1.8589,  0.4941, -0.2369,\n",
      "          -1.3683]]], device='mps:0')\n",
      "\n",
      "====================Entering Norm.LayerNorm====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -2.426/2.874\n",
      "Full tensor content:\n",
      "tensor([[[-1.0500, -0.1534, -0.4999,  0.6605, -0.2287,  1.7829, -0.6294,\n",
      "           0.1742],\n",
      "         [ 0.2729, -0.4710,  0.5910, -0.7012,  0.1361,  1.4918,  0.4335,\n",
      "          -1.0387],\n",
      "         [ 0.2086, -0.5668, -0.1605,  0.2868, -2.2282, -0.5542,  0.7998,\n",
      "          -1.2815],\n",
      "         [ 0.7306,  0.0831, -0.8461,  0.5909, -0.3932,  0.3018,  0.5686,\n",
      "           0.6200],\n",
      "         [-0.2238, -0.7060,  1.8229,  0.7945, -0.7008, -0.6020, -1.5264,\n",
      "           0.0326],\n",
      "         [ 2.8740, -0.0607,  1.1707, -0.3802, -0.5758, -1.7617,  0.4685,\n",
      "          -1.4737],\n",
      "         [ 0.5714,  0.9737,  0.9463,  0.8130,  0.3562,  0.7989,  0.6797,\n",
      "          -0.4417],\n",
      "         [-0.2639, -0.4763,  0.6674,  0.1654,  1.3941,  0.4235,  1.8382,\n",
      "          -1.7479],\n",
      "         [-1.2054,  0.2419,  0.3639,  0.5018, -1.1669,  0.8820,  0.5607,\n",
      "          -0.3508],\n",
      "         [ 0.8573, -0.7545,  0.3449, -1.1696, -0.1534,  0.6343, -1.2354,\n",
      "          -1.1734],\n",
      "         [-0.4871, -0.1617,  0.3188, -0.3874,  1.2219,  0.1743,  0.4443,\n",
      "           0.9441],\n",
      "         [-0.4933, -0.9870,  1.2818, -1.3832,  0.1202, -2.1615, -1.0478,\n",
      "           0.4527],\n",
      "         [-0.1114, -0.1225,  1.7615, -0.7113, -0.6273,  0.4197, -0.7589,\n",
      "           0.5602],\n",
      "         [ 0.2764,  0.6196,  0.8415, -0.7104,  0.0316,  0.6711,  2.0246,\n",
      "          -0.0484],\n",
      "         [ 0.7228,  0.2619, -0.3030,  0.0460,  0.9620, -1.0834,  2.1172,\n",
      "           0.6024],\n",
      "         [ 0.7484,  1.0675, -0.6769,  0.0952,  1.2399, -0.5372, -0.2298,\n",
      "           2.2404]],\n",
      "\n",
      "        [[-1.4238, -0.6464, -0.3003, -0.3567, -0.1210,  1.1321, -0.6748,\n",
      "          -0.6645],\n",
      "         [ 0.6674,  1.8005,  2.4290, -0.8607, -0.4747, -0.1016,  2.0740,\n",
      "          -0.2857],\n",
      "         [-1.4213,  1.1788,  0.1805, -0.7369,  0.1761,  0.1399,  0.0629,\n",
      "          -2.0777],\n",
      "         [-0.7911,  0.9655, -1.4965,  1.1998,  0.9893, -1.3574,  0.5672,\n",
      "           1.2281],\n",
      "         [ 0.9243,  0.5867, -0.3333, -1.9671,  0.2039, -1.2445,  1.2913,\n",
      "           0.6352],\n",
      "         [ 0.4935,  0.5510, -0.3564, -0.0378,  0.2247, -1.0182, -0.0616,\n",
      "          -0.1109],\n",
      "         [-0.0190, -0.9928, -0.4837,  1.1592,  1.4838, -0.5304,  1.0039,\n",
      "          -0.4426],\n",
      "         [ 1.2183,  1.7060, -0.5656, -0.9355, -1.5974, -1.7481, -0.7165,\n",
      "           0.1227],\n",
      "         [ 0.0229, -0.4363, -0.8647,  0.2886,  1.9674,  1.3417,  1.7155,\n",
      "          -0.0825],\n",
      "         [-2.4260, -2.1571, -0.2377,  0.4924, -0.4597,  0.0410, -0.6809,\n",
      "           0.5661],\n",
      "         [-1.0427,  1.7310,  0.0256, -0.4507, -0.3847,  0.0815,  0.1870,\n",
      "           0.8699],\n",
      "         [ 0.0745, -0.5198, -0.3924,  1.1076,  0.0591, -0.3113,  0.0696,\n",
      "           0.0575],\n",
      "         [-0.4326,  1.3485,  0.0088, -0.7957,  0.5488, -0.1240,  1.1093,\n",
      "           0.2768],\n",
      "         [ 0.7184,  0.8009, -0.6520, -0.5444,  1.3001,  1.3125, -0.4642,\n",
      "           1.3442],\n",
      "         [ 1.2597, -0.0219,  0.3184,  0.5466, -0.2180, -0.3436,  0.1015,\n",
      "          -0.9529],\n",
      "         [ 1.0704,  0.6365,  1.0409,  0.3957,  1.8589,  0.4941, -0.2369,\n",
      "          -1.3683]]], device='mps:0')\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -2.380/2.321\n",
      "Full tensor content:\n",
      "tensor([[[-1.2784, -0.1940, -0.6131,  0.7904, -0.2852,  2.1478, -0.7697,\n",
      "           0.2022],\n",
      "         [ 0.2425, -0.7402,  0.6628, -1.0443,  0.0618,  1.8527,  0.4547,\n",
      "          -1.4901],\n",
      "         [ 0.7163, -0.1440,  0.3068,  0.8031, -1.9873, -0.1300,  1.3722,\n",
      "          -0.9369],\n",
      "         [ 0.9948, -0.2353, -2.0007,  0.7293, -1.1402,  0.1802,  0.6871,\n",
      "           0.7847],\n",
      "         [-0.0877, -0.5847,  2.0213,  0.9616, -0.5793, -0.4776, -1.4301,\n",
      "           0.1765],\n",
      "         [ 2.0350, -0.0669,  0.8151, -0.2957, -0.4357, -1.2851,  0.3121,\n",
      "          -1.0788],\n",
      "         [-0.0364,  0.8941,  0.8308,  0.5223, -0.5343,  0.4897,  0.2141,\n",
      "          -2.3802],\n",
      "         [-0.4893, -0.6915,  0.3973, -0.0806,  1.0892,  0.1652,  1.5120,\n",
      "          -1.9022],\n",
      "         [-1.5839,  0.3526,  0.5157,  0.7002, -1.5323,  1.2090,  0.7791,\n",
      "          -0.4404],\n",
      "         [ 1.4682, -0.5229,  0.8352, -1.0357,  0.2197,  1.1928, -1.1170,\n",
      "          -1.0403],\n",
      "         [-1.3094, -0.7378,  0.1061, -1.1343,  1.6923, -0.1478,  0.3265,\n",
      "           1.2044],\n",
      "         [ 0.0329, -0.4450,  1.7510, -0.8285,  0.6267, -1.5818, -0.5038,\n",
      "           0.9485],\n",
      "         [-0.2042, -0.2181,  2.1473, -0.9575, -0.8520,  0.4627, -1.0172,\n",
      "           0.6390],\n",
      "         [-0.2489,  0.2082,  0.5039, -1.5633, -0.5750,  0.2768,  2.0797,\n",
      "          -0.6815],\n",
      "         [ 0.3475, -0.1740, -0.8134, -0.4184,  0.6182, -1.6965,  1.9255,\n",
      "           0.2112],\n",
      "         [ 0.2703,  0.6086, -1.2408, -0.4222,  0.7914, -1.0927, -0.7668,\n",
      "           1.8521]],\n",
      "\n",
      "        [[-1.5344, -0.3895,  0.1202,  0.0372,  0.3843,  2.2296, -0.4313,\n",
      "          -0.4161],\n",
      "         [ 0.0095,  0.9544,  1.4785, -1.2649, -0.9429, -0.6318,  1.1825,\n",
      "          -0.7853],\n",
      "         [-1.1396,  1.5321,  0.5063, -0.4364,  0.5017,  0.4646,  0.3854,\n",
      "          -1.8141],\n",
      "         [-0.8677,  0.7296, -1.5091,  0.9427,  0.7512, -1.3826,  0.3674,\n",
      "           0.9684],\n",
      "         [ 0.8678,  0.5467, -0.3286, -1.8829,  0.1825, -1.1954,  1.2170,\n",
      "           0.5928],\n",
      "         [ 1.1341,  1.2566, -0.6744,  0.0036,  0.5622, -2.0829, -0.0472,\n",
      "          -0.1520],\n",
      "         [-0.1908, -1.3081, -0.7240,  1.1610,  1.5335, -0.7775,  0.9828,\n",
      "          -0.6769],\n",
      "         [ 1.3111,  1.7282, -0.2148, -0.5312, -1.0974, -1.2262, -0.3438,\n",
      "           0.3740],\n",
      "         [-0.4806, -0.9490, -1.3859, -0.2096,  1.5028,  0.8645,  1.2458,\n",
      "          -0.5881],\n",
      "         [-1.7258, -1.4706,  0.3513,  1.0442,  0.1405,  0.6157, -0.0694,\n",
      "           1.1141],\n",
      "         [-1.4645,  2.0080, -0.1271, -0.7233, -0.6408, -0.0571,  0.0749,\n",
      "           0.9299],\n",
      "         [ 0.1201, -1.1460, -0.8745,  2.3212,  0.0874, -0.7019,  0.1096,\n",
      "           0.0840],\n",
      "         [-0.9806,  1.6064, -0.3395, -1.5079,  0.4449, -0.5323,  1.2590,\n",
      "           0.0498],\n",
      "         [ 0.2914,  0.3909, -1.3622, -1.2324,  0.9933,  1.0083, -1.1356,\n",
      "           1.0465],\n",
      "         [ 1.9125, -0.1762,  0.3783,  0.7503, -0.4958, -0.7005,  0.0249,\n",
      "          -1.6936],\n",
      "         [ 0.6466,  0.1661,  0.6140, -0.1005,  1.5196,  0.0085, -0.8008,\n",
      "          -2.0535]]], device='mps:0')\n",
      "====================Exiting Norm.LayerNorm====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -2.380/2.321\n",
      "Full tensor content:\n",
      "tensor([[[-1.2784, -0.1940, -0.6131,  0.7904, -0.2852,  2.1478, -0.7697,\n",
      "           0.2022],\n",
      "         [ 0.2425, -0.7402,  0.6628, -1.0443,  0.0618,  1.8527,  0.4547,\n",
      "          -1.4901],\n",
      "         [ 0.7163, -0.1440,  0.3068,  0.8031, -1.9873, -0.1300,  1.3722,\n",
      "          -0.9369],\n",
      "         [ 0.9948, -0.2353, -2.0007,  0.7293, -1.1402,  0.1802,  0.6871,\n",
      "           0.7847],\n",
      "         [-0.0877, -0.5847,  2.0213,  0.9616, -0.5793, -0.4776, -1.4301,\n",
      "           0.1765],\n",
      "         [ 2.0350, -0.0669,  0.8151, -0.2957, -0.4357, -1.2851,  0.3121,\n",
      "          -1.0788],\n",
      "         [-0.0364,  0.8941,  0.8308,  0.5223, -0.5343,  0.4897,  0.2141,\n",
      "          -2.3802],\n",
      "         [-0.4893, -0.6915,  0.3973, -0.0806,  1.0892,  0.1652,  1.5120,\n",
      "          -1.9022],\n",
      "         [-1.5839,  0.3526,  0.5157,  0.7002, -1.5323,  1.2090,  0.7791,\n",
      "          -0.4404],\n",
      "         [ 1.4682, -0.5229,  0.8352, -1.0357,  0.2197,  1.1928, -1.1170,\n",
      "          -1.0403],\n",
      "         [-1.3094, -0.7378,  0.1061, -1.1343,  1.6923, -0.1478,  0.3265,\n",
      "           1.2044],\n",
      "         [ 0.0329, -0.4450,  1.7510, -0.8285,  0.6267, -1.5818, -0.5038,\n",
      "           0.9485],\n",
      "         [-0.2042, -0.2181,  2.1473, -0.9575, -0.8520,  0.4627, -1.0172,\n",
      "           0.6390],\n",
      "         [-0.2489,  0.2082,  0.5039, -1.5633, -0.5750,  0.2768,  2.0797,\n",
      "          -0.6815],\n",
      "         [ 0.3475, -0.1740, -0.8134, -0.4184,  0.6182, -1.6965,  1.9255,\n",
      "           0.2112],\n",
      "         [ 0.2703,  0.6086, -1.2408, -0.4222,  0.7914, -1.0927, -0.7668,\n",
      "           1.8521]],\n",
      "\n",
      "        [[-1.5344, -0.3895,  0.1202,  0.0372,  0.3843,  2.2296, -0.4313,\n",
      "          -0.4161],\n",
      "         [ 0.0095,  0.9544,  1.4785, -1.2649, -0.9429, -0.6318,  1.1825,\n",
      "          -0.7853],\n",
      "         [-1.1396,  1.5321,  0.5063, -0.4364,  0.5017,  0.4646,  0.3854,\n",
      "          -1.8141],\n",
      "         [-0.8677,  0.7296, -1.5091,  0.9427,  0.7512, -1.3826,  0.3674,\n",
      "           0.9684],\n",
      "         [ 0.8678,  0.5467, -0.3286, -1.8829,  0.1825, -1.1954,  1.2170,\n",
      "           0.5928],\n",
      "         [ 1.1341,  1.2566, -0.6744,  0.0036,  0.5622, -2.0829, -0.0472,\n",
      "          -0.1520],\n",
      "         [-0.1908, -1.3081, -0.7240,  1.1610,  1.5335, -0.7775,  0.9828,\n",
      "          -0.6769],\n",
      "         [ 1.3111,  1.7282, -0.2148, -0.5312, -1.0974, -1.2262, -0.3438,\n",
      "           0.3740],\n",
      "         [-0.4806, -0.9490, -1.3859, -0.2096,  1.5028,  0.8645,  1.2458,\n",
      "          -0.5881],\n",
      "         [-1.7258, -1.4706,  0.3513,  1.0442,  0.1405,  0.6157, -0.0694,\n",
      "           1.1141],\n",
      "         [-1.4645,  2.0080, -0.1271, -0.7233, -0.6408, -0.0571,  0.0749,\n",
      "           0.9299],\n",
      "         [ 0.1201, -1.1460, -0.8745,  2.3212,  0.0874, -0.7019,  0.1096,\n",
      "           0.0840],\n",
      "         [-0.9806,  1.6064, -0.3395, -1.5079,  0.4449, -0.5323,  1.2590,\n",
      "           0.0498],\n",
      "         [ 0.2914,  0.3909, -1.3622, -1.2324,  0.9933,  1.0083, -1.1356,\n",
      "           1.0465],\n",
      "         [ 1.9125, -0.1762,  0.3783,  0.7503, -0.4958, -0.7005,  0.0249,\n",
      "          -1.6936],\n",
      "         [ 0.6466,  0.1661,  0.6140, -0.1005,  1.5196,  0.0085, -0.8008,\n",
      "          -2.0535]]], device='mps:0', grad_fn=<AddBackward0>)\n",
      "====================Exiting Norm.forward====================\n",
      "CPU times: user 109 ms, sys: 66.2 ms, total: 175 ms\n",
      "Wall time: 322 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# LayerNorm\n",
    "module = Norm(cfg.dim, 'LayerNorm').to(cfg.device)\n",
    "module.enable_logging()\n",
    "\n",
    "# you can also have it optionally print out all tensors in full\n",
    "module.enable_full_tensor_printing()\n",
    "# i recommend only doing this with very small toy values for your hyperparameters, otherwise this gets too big\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('CosineNorm')\n",
    "#module.disable_function_logging('LayerNorm')\n",
    "#module.disable_function_logging('RMSNorm')\n",
    "\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "output = module(x)\n",
    "module.disable_logging()\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e190f3de-37fd-442b-bfb1-6a090115fc75",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a89e1e8f-cedd-4885-ad50-934827ed045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.attention import SelfAttention, PrecomputeRotaryFrequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e8f27b4-6d1f-4fcd-99b0-2284e65d6b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.19K\n",
      "SelfAttention(\n",
      "  (Wq): Linear(in_features=8, out_features=8, bias=False)\n",
      "  (Wk): Linear(in_features=8, out_features=4, bias=False)\n",
      "  (Wv): Linear(in_features=8, out_features=4, bias=False)\n",
      "  (Wo): Linear(in_features=8, out_features=8, bias=False)\n",
      ")\n",
      "\n",
      "====================Entering SelfAttention.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -2.513/2.460\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.000/0.991\n",
      "Tensor 'mask' shape: torch.Size([16, 16]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "====================Entering SelfAttention.apply_rotary_pos_emb====================\n",
      "Inputs:\n",
      "Tensor 'q' shape: torch.Size([2, 16, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.140/1.784\n",
      "Tensor 'k' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.301/1.256\n",
      "Dict 'freqs_cis':\n",
      "    Tensor 'freqs_cis[cos]' shape: torch.Size([1, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs_cis[sin]' shape: torch.Size([1, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.000/0.991\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "\n",
      "====================Entering SelfAttention.rotate_half====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 16, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.140/1.784\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([2, 16, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.784/1.297\n",
      "====================Exiting SelfAttention.rotate_half====================\n",
      "\n",
      "====================Entering SelfAttention.rotate_half====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.301/1.256\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.256/1.301\n",
      "====================Exiting SelfAttention.rotate_half====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([2, 16, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.880/1.588\n",
      "Tensor 'output[1]' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.404/1.342\n",
      "====================Exiting SelfAttention.apply_rotary_pos_emb====================\n",
      "\n",
      "====================Entering SelfAttention.match_headcount====================\n",
      "Inputs:\n",
      "Tensor 'k' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.404/1.342\n",
      "Tensor 'v' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.872/1.279\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([2, 16, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.404/1.342\n",
      "Tensor 'output[1]' shape: torch.Size([2, 16, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -1.872/1.279\n",
      "====================Exiting SelfAttention.match_headcount====================\n",
      "\n",
      "====================Entering SelfAttention.flash_attention====================\n",
      "Inputs:\n",
      "Tensor 'q' shape: torch.Size([2, 2, 16, 4]), dtype: torch.float32, device: mps:0, min/max: -1.880/1.588\n",
      "Tensor 'k' shape: torch.Size([2, 2, 16, 4]), dtype: torch.float32, device: mps:0, min/max: -1.404/1.342\n",
      "Tensor 'v' shape: torch.Size([2, 2, 16, 4]), dtype: torch.float32, device: mps:0, min/max: -1.872/1.279\n",
      "Tensor 'mask' shape: torch.Size([16, 16]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([2, 2, 16, 4]), dtype: torch.float32, device: mps:0, min/max: -1.058/1.343\n",
      "====================Exiting SelfAttention.flash_attention====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -0.473/0.565\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting SelfAttention.forward====================\n",
      "CPU times: user 149 ms, sys: 23.6 ms, total: 172 ms\n",
      "Wall time: 208 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# first up let's look at training\n",
    "\n",
    "# Create an instance of multi-head self-attention\n",
    "module = SelfAttention(cfg.dim, cfg.head_dim, cfg.num_q_heads, cfg.num_kv_heads, cfg.max_seq_len, cfg.linear_bias, device=cfg.device)\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "\n",
    "# Initially, logging is disabled by default\n",
    "module.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('apply_precompute_freqs')\n",
    "#module.disable_function_logging('reshape_for_broadcast')\n",
    "#module.disable_function_logging('match_headcount')\n",
    "#module.disable_function_logging('attend')\n",
    "#module.disable_function_logging('calc_output')\n",
    "\n",
    "# precompute RoPE frequencies, causal mask, and dummy input data\n",
    "if cfg.pos_enc_type == 'RoPE':\n",
    "    precompute_freqs = PrecomputeRotaryFrequencies(cfg.head_dim, cfg.max_seq_len, cfg.theta, cfg.device)\n",
    "    freqs = precompute_freqs()\n",
    "else:\n",
    "    freqs = None\n",
    "mask = torch.ones(cfg.max_seq_len, cfg.max_seq_len, dtype=torch.bool, device=cfg.device).triu(diagonal=1)\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "output = module(x, freqs, mask, training=True)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, freqs, mask, x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e7e034c-c5a6-43ac-b6d2-bbb71f9965ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================Entering SelfAttention.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 1, 8]), dtype: torch.float32, device: mps:0, min/max: -1.696/1.909\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.000/0.991\n",
      "Tensor 'mask' shape: torch.Size([1, 9]), dtype: torch.bool, device: mps:0, min/max: 0.000/0.000\n",
      "Integer 'cache_len': Value=8\n",
      "Dict 'kv_cache':\n",
      "    Tensor 'kv_cache[k]' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "    Tensor 'kv_cache[v]' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "Bool 'training': Value=False\n",
      "\n",
      "====================Entering SelfAttention.apply_rotary_pos_emb====================\n",
      "Inputs:\n",
      "Tensor 'q' shape: torch.Size([2, 1, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -0.624/0.745\n",
      "Tensor 'k' shape: torch.Size([2, 1, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.006/1.030\n",
      "Dict 'freqs_cis':\n",
      "    Tensor 'freqs_cis[cos]' shape: torch.Size([1, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs_cis[sin]' shape: torch.Size([1, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.000/0.991\n",
      "Integer 'cache_len': Value=8\n",
      "\n",
      "====================Entering SelfAttention.rotate_half====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 1, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -0.624/0.745\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([2, 1, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -0.624/0.745\n",
      "====================Exiting SelfAttention.rotate_half====================\n",
      "\n",
      "====================Entering SelfAttention.rotate_half====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 1, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.006/1.030\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([2, 1, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.030/0.285\n",
      "====================Exiting SelfAttention.rotate_half====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([2, 1, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -0.603/0.692\n",
      "Tensor 'output[1]' shape: torch.Size([2, 1, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.400/1.041\n",
      "====================Exiting SelfAttention.apply_rotary_pos_emb====================\n",
      "\n",
      "====================Entering SelfAttention.match_headcount====================\n",
      "Inputs:\n",
      "Tensor 'k' shape: torch.Size([2, 9, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.400/1.041\n",
      "Tensor 'v' shape: torch.Size([2, 9, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.249/0.636\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([2, 9, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -0.400/1.041\n",
      "Tensor 'output[1]' shape: torch.Size([2, 9, 2, 4]), dtype: torch.float32, device: mps:0, min/max: -0.249/0.636\n",
      "====================Exiting SelfAttention.match_headcount====================\n",
      "\n",
      "====================Entering SelfAttention.flash_attention====================\n",
      "Inputs:\n",
      "Tensor 'q' shape: torch.Size([2, 2, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.603/0.692\n",
      "Tensor 'k' shape: torch.Size([2, 2, 9, 4]), dtype: torch.float32, device: mps:0, min/max: -0.400/1.041\n",
      "Tensor 'v' shape: torch.Size([2, 2, 9, 4]), dtype: torch.float32, device: mps:0, min/max: -0.249/0.636\n",
      "Tensor 'mask' shape: torch.Size([1, 9]), dtype: torch.bool, device: mps:0, min/max: 0.000/0.000\n",
      "Bool 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([2, 2, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.029/0.073\n",
      "====================Exiting SelfAttention.flash_attention====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([2, 1, 8]), dtype: torch.float32, device: mps:0, min/max: -0.053/0.033\n",
      "Dict 'output[1]':\n",
      "    Tensor 'output[1][k]' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.400/1.041\n",
      "    Tensor 'output[1][v]' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.249/0.636\n",
      "====================Exiting SelfAttention.forward====================\n",
      "CPU times: user 116 ms, sys: 16.5 ms, total: 133 ms\n",
      "Wall time: 159 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# now let's do it for inference\n",
    "\n",
    "module = SelfAttention(cfg.dim, cfg.head_dim, cfg.num_q_heads, cfg.num_kv_heads, cfg.max_seq_len, cfg.linear_bias, device=cfg.device)\n",
    "module.enable_logging()\n",
    "#module.disable_function_logging('apply_precompute_freqs')\n",
    "#module.disable_function_logging('reshape_for_broadcast')\n",
    "#module.disable_function_logging('match_headcount')\n",
    "#module.disable_function_logging('attend')\n",
    "#module.disable_function_logging('calc_output')\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "# precompute RoPE frequencies, causal mask, and dummy input data\n",
    "if cfg.pos_enc_type == 'RoPE':\n",
    "    precompute_freqs = PrecomputeRotaryFrequencies(cfg.head_dim, cfg.max_seq_len, cfg.theta, cfg.device)\n",
    "    freqs = precompute_freqs()\n",
    "else:\n",
    "    freqs = None\n",
    "mask = torch.ones(cfg.max_seq_len, cfg.max_seq_len, dtype=torch.bool, device=cfg.device).triu(diagonal=1)\n",
    "# setting up for kv caching\n",
    "cache_len = random.randint(1, cfg.max_seq_len // 2)\n",
    "seq_len = cache_len + 1\n",
    "kv_cache = {\n",
    "    'k': torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device),\n",
    "    'v': torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device)\n",
    "}\n",
    "# need to extend the mask with zeros for the cached values\n",
    "mask = torch.nn.functional.pad(mask[:1, :1], (cache_len, 0, 0, 0), value=False).bool()\n",
    "x = torch.randn(tcfg.micro_batch_size,1,cfg.dim).to(cfg.device)\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "output = module(x, freqs, mask, cache_len, kv_cache)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, freqs, mask, cache_len, seq_len, kv_cache, x, output#, context_chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb308c5-b578-46f2-86ae-bfa6800be641",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7736b685-f941-4182-a5b7-4731cce706b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.mlp import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c62e49e9-2189-4269-968e-1df99469dc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.50K\n",
      "MLP(\n",
      "  (Wup): Linear(in_features=8, out_features=21, bias=False)\n",
      "  (Wgate): Linear(in_features=8, out_features=21, bias=False)\n",
      "  (Wdown): Linear(in_features=21, out_features=8, bias=False)\n",
      "  (nonlinearity): GELU(approximate='none')\n",
      ")\n",
      "\n",
      "====================Entering MLP.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -2.804/2.305\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -0.500/0.370\n",
      "====================Exiting MLP.forward====================\n",
      "CPU times: user 33.6 ms, sys: 8.19 ms, total: 41.8 ms\n",
      "Wall time: 103 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# GeGLU\n",
    "module = MLP(\n",
    "    cfg.dim, \n",
    "    int(cfg.dim * cfg.mlp_hidden_mult * 2/3), \n",
    "    cfg.dim, \n",
    "    'GeLU', \n",
    "    gated=True, \n",
    "    bias=cfg.linear_bias, \n",
    "    dropout_rate = 0.1\n",
    ").to(cfg.device)\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "module.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "output = module(x, training=True)\n",
    "module.disable_logging()\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0748fa3-3230-4dd7-a768-1256ea72e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.55K\n",
      "MLP(\n",
      "  (Wup): Linear(in_features=8, out_features=32, bias=True)\n",
      "  (Wdown): Linear(in_features=32, out_features=8, bias=True)\n",
      "  (nonlinearity): ReLU()\n",
      ")\n",
      "\n",
      "====================Entering MLP.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -3.147/2.986\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -1.142/0.955\n",
      "====================Exiting MLP.forward====================\n",
      "CPU times: user 39.1 ms, sys: 6.14 ms, total: 45.2 ms\n",
      "Wall time: 50.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# not gated, testing every other nonlinearity\n",
    "module = MLP(\n",
    "    cfg.dim, \n",
    "    cfg.dim * cfg.mlp_hidden_mult, \n",
    "    cfg.dim, \n",
    "    'ReLU', \n",
    "    gated=False, \n",
    "    bias=True, \n",
    "    dropout_rate = 0.1\n",
    ").to(cfg.device)\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "module.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "output = module(x, training=True)\n",
    "module.disable_logging()\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3a502f-4646-4a02-9412-372482af9fa0",
   "metadata": {},
   "source": [
    "# ResidualLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a21d708-af47-4f32-b111-08efedc584f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.layer import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c886661d-5a26-4787-93d1-c3c8c6b1c02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.73K\n",
      "Layer(\n",
      "  (pre_attn_norm): Norm()\n",
      "  (attn): SelfAttention(\n",
      "    (Wq): Linear(in_features=8, out_features=8, bias=False)\n",
      "    (Wk): Linear(in_features=8, out_features=4, bias=False)\n",
      "    (Wv): Linear(in_features=8, out_features=4, bias=False)\n",
      "    (Wo): Linear(in_features=8, out_features=8, bias=False)\n",
      "  )\n",
      "  (pre_mlp_norm): Norm()\n",
      "  (mlp): MLP(\n",
      "    (Wup): Linear(in_features=8, out_features=21, bias=False)\n",
      "    (Wgate): Linear(in_features=8, out_features=21, bias=False)\n",
      "    (Wdown): Linear(in_features=21, out_features=8, bias=False)\n",
      "    (nonlinearity): SiLU()\n",
      "  )\n",
      ")\n",
      "\n",
      "====================Entering Layer.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -2.560/2.798\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.000/0.991\n",
      "Tensor 'mask' shape: torch.Size([16, 16]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "====================Entering Layer.attn_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -2.560/2.798\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.000/0.991\n",
      "Tensor 'mask' shape: torch.Size([16, 16]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -0.593/0.627\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.attn_connect====================\n",
      "\n",
      "====================Entering Layer.mlp_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -2.634/2.893\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -0.273/0.370\n",
      "====================Exiting Layer.mlp_connect====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -2.679/3.080\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.forward====================\n",
      "CPU times: user 33.2 ms, sys: 9.2 ms, total: 42.4 ms\n",
      "Wall time: 48.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TRAINING\n",
    "module = Layer(cfg).to(cfg.device)\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "#module.disable_function_logging('attn_connect')\n",
    "#module.disable_function_logging('mlp_connect')\n",
    "### enabling printing for sub-modules\n",
    "#module.pre_attn_norm.enable_logging()\n",
    "#module.attn.enable_logging()\n",
    "#module.post_attn_norm.enable_logging()\n",
    "#module.pre_mlp_norm.enable_logging()\n",
    "#module.mlp.enable_logging()\n",
    "#module.post_mlp_norm.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "# precompute RoPE frequencies, causal mask, and dummy input data\n",
    "if cfg.pos_enc_type == 'RoPE':\n",
    "    precompute_freqs = PrecomputeRotaryFrequencies(cfg.head_dim, cfg.max_seq_len, cfg.theta, cfg.device)\n",
    "    freqs = precompute_freqs()\n",
    "else:\n",
    "    freqs = None\n",
    "mask = torch.ones(cfg.max_seq_len, cfg.max_seq_len, dtype=torch.bool, device=cfg.device).triu(diagonal=1)\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "\n",
    "output = module(x, freqs, mask, training=True)\n",
    "module.disable_logging()\n",
    "del module,freqs, mask, x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f534ab4-a150-42db-8c89-5e8d3d0c06a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.73K\n",
      "Layer(\n",
      "  (pre_attn_norm): Norm()\n",
      "  (attn): SelfAttention(\n",
      "    (Wq): Linear(in_features=8, out_features=8, bias=False)\n",
      "    (Wk): Linear(in_features=8, out_features=4, bias=False)\n",
      "    (Wv): Linear(in_features=8, out_features=4, bias=False)\n",
      "    (Wo): Linear(in_features=8, out_features=8, bias=False)\n",
      "  )\n",
      "  (pre_mlp_norm): Norm()\n",
      "  (mlp): MLP(\n",
      "    (Wup): Linear(in_features=8, out_features=21, bias=False)\n",
      "    (Wgate): Linear(in_features=8, out_features=21, bias=False)\n",
      "    (Wdown): Linear(in_features=21, out_features=8, bias=False)\n",
      "    (nonlinearity): SiLU()\n",
      "  )\n",
      ")\n",
      "\n",
      "====================Entering Layer.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 1, 8]), dtype: torch.float32, device: mps:0, min/max: -1.003/1.721\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.000/0.991\n",
      "Tensor 'mask' shape: torch.Size([1, 9]), dtype: torch.bool, device: mps:0, min/max: 0.000/0.000\n",
      "Integer 'cache_len': Value=8\n",
      "Dict 'kv_cache':\n",
      "    Tensor 'kv_cache[k]' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "    Tensor 'kv_cache[v]' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "Bool 'training': Value=False\n",
      "\n",
      "====================Entering Layer.attn_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 1, 8]), dtype: torch.float32, device: mps:0, min/max: -1.003/1.721\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.000/0.991\n",
      "Tensor 'mask' shape: torch.Size([1, 9]), dtype: torch.bool, device: mps:0, min/max: 0.000/0.000\n",
      "Integer 'cache_len': Value=8\n",
      "Dict 'kv_cache':\n",
      "    Tensor 'kv_cache[k]' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "    Tensor 'kv_cache[v]' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "Bool 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([2, 1, 8]), dtype: torch.float32, device: mps:0, min/max: -0.049/0.044\n",
      "Dict 'output[1]':\n",
      "    Tensor 'output[1][k]' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.685/1.089\n",
      "    Tensor 'output[1][v]' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.785/0.690\n",
      "====================Exiting Layer.attn_connect====================\n",
      "\n",
      "====================Entering Layer.mlp_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 1, 8]), dtype: torch.float32, device: mps:0, min/max: -1.024/1.687\n",
      "Bool 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([2, 1, 8]), dtype: torch.float32, device: mps:0, min/max: -0.241/0.172\n",
      "====================Exiting Layer.mlp_connect====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([2, 1, 8]), dtype: torch.float32, device: mps:0, min/max: -0.905/1.820\n",
      "Dict 'output[1]':\n",
      "    Tensor 'output[1][k]' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.685/1.089\n",
      "    Tensor 'output[1][v]' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.785/0.690\n",
      "====================Exiting Layer.forward====================\n",
      "CPU times: user 59.5 ms, sys: 12.6 ms, total: 72.2 ms\n",
      "Wall time: 86.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# INFERENCE\n",
    "module = Layer(cfg).to(cfg.device)\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "#module.disable_function_logging('attn_connect')\n",
    "#module.disable_function_logging('mlp_connect')\n",
    "#module.pre_attn_norm.enable_logging()\n",
    "#module.attn.enable_logging()\n",
    "#module.post_attn_norm.enable_logging()\n",
    "#module.pre_mlp_norm.enable_logging()\n",
    "#module.mlp.enable_logging()\n",
    "#module.post_mlp_norm.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "# precompute RoPE frequencies, causal mask, and dummy input data\n",
    "if cfg.pos_enc_type == 'RoPE':\n",
    "    precompute_freqs = PrecomputeRotaryFrequencies(cfg.head_dim, cfg.max_seq_len, cfg.theta, cfg.device)\n",
    "    freqs = precompute_freqs()\n",
    "else:\n",
    "    freqs = None\n",
    "mask = torch.ones(cfg.max_seq_len, cfg.max_seq_len, dtype=torch.bool, device=cfg.device).triu(diagonal=1)\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "# setting up for kv caching\n",
    "cache_len = random.randint(1, cfg.max_seq_len // 2)\n",
    "seq_len = cache_len + 1 \n",
    "kv_cache = {\n",
    "    'k': torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device),\n",
    "    'v': torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device)\n",
    "}\n",
    "# need to extend the mask with zeros for the cached values\n",
    "mask = torch.nn.functional.pad(mask[:1, :1],(cache_len, 0, 0, 0), value=False).bool()\n",
    "x = torch.randn(tcfg.micro_batch_size,1,cfg.dim).to(cfg.device)\n",
    "\n",
    "output = module(x, freqs, mask, cache_len, kv_cache)\n",
    "module.disable_logging()\n",
    "del module, freqs, mask, cache_len, seq_len, kv_cache, x, output#, context_chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c677ac2b-06d0-4895-b718-2bc664613c98",
   "metadata": {},
   "source": [
    "# Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "655a1fec-4e32-4c7a-86e0-1390a03e88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ccc88d4-c650-43d3-85ee-e68152cb2e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.02M\n",
      "Model(\n",
      "  (precompute_freqs): PrecomputeRotaryFrequencies()\n",
      "  (token_embedder): Embedding(2048, 8)\n",
      "  (layers): ModuleList(\n",
      "    (0): Layer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): SelfAttention(\n",
      "        (Wq): Linear(in_features=8, out_features=8, bias=False)\n",
      "        (Wk): Linear(in_features=8, out_features=4, bias=False)\n",
      "        (Wv): Linear(in_features=8, out_features=4, bias=False)\n",
      "        (Wo): Linear(in_features=8, out_features=8, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wup): Linear(in_features=8, out_features=21, bias=False)\n",
      "        (Wgate): Linear(in_features=8, out_features=21, bias=False)\n",
      "        (Wdown): Linear(in_features=21, out_features=8, bias=False)\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (output): Linear(in_features=8, out_features=2048, bias=False)\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "\n",
      "====================Entering Model.forward====================\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([2, 16]), dtype: torch.int64, device: mps:0, min/max: 13.000/506.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Tensor 'target_token_ids' shape: torch.Size([2, 16]), dtype: torch.int64, device: mps:0, min/max: 16.000/504.000\n",
      "\n",
      "====================Entering Layer.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -0.190/0.174\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.000/0.991\n",
      "Tensor 'mask' shape: torch.Size([16, 16]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "====================Entering Layer.attn_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -0.190/0.174\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.990/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -1.000/0.991\n",
      "Tensor 'mask' shape: torch.Size([16, 16]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -0.005/0.008\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.attn_connect====================\n",
      "\n",
      "====================Entering Layer.mlp_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -0.190/0.174\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -0.000/0.000\n",
      "====================Exiting Layer.mlp_connect====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -0.190/0.174\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.forward====================\n",
      "\n",
      "====================Entering Norm.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -0.190/0.174\n",
      "\n",
      "====================Entering Norm.RMSNorm====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -0.190/0.174\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -2.423/2.522\n",
      "====================Exiting Norm.RMSNorm====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([2, 16, 8]), dtype: torch.float32, device: mps:0, min/max: -2.423/2.522\n",
      "====================Exiting Norm.forward====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([2, 16, 2048]), dtype: torch.float32, device: mps:0, min/max: -0.248/0.257\n",
      "Tensor 'output[1]' shape: torch.Size([]), dtype: torch.float32, device: mps:0, min/max: 7.614/7.614\n",
      "====================Exiting Model.forward====================\n",
      "tensor(7.6137, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "CPU times: user 123 ms, sys: 18.3 ms, total: 142 ms\n",
      "Wall time: 177 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TRAINING\n",
    "module = Model(cfg).to(cfg.device)\n",
    "print(\"number of parameters: %.2fM\" % (module.get_num_params()/1e6,))\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "### enabling printing for sub-modules\n",
    "#module.precompute_freqs.enable_logging() # only un-comment this line if using RoPE\n",
    "#module.layers[0].enable_logging()\n",
    "for i in range(cfg.num_layers):\n",
    "    module.layers[i].enable_logging()\n",
    "module.final_norm.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "input_token_ids = torch.randint(tokenizer.vocab_len, (tcfg.micro_batch_size, cfg.max_seq_len)).to(cfg.device)\n",
    "target_token_ids = torch.randint(tokenizer.vocab_len, (tcfg.micro_batch_size, cfg.max_seq_len)).to(cfg.device)\n",
    "\n",
    "output, loss = module(input_token_ids, target_token_ids=target_token_ids)\n",
    "print(loss)\n",
    "del module, input_token_ids, target_token_ids, output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "316598a6-1ba9-4a26-962f-16c0802a698e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.02M\n",
      "Model(\n",
      "  (precompute_freqs): PrecomputeRotaryFrequencies()\n",
      "  (token_embedder): Embedding(2048, 8)\n",
      "  (layers): ModuleList(\n",
      "    (0): Layer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): SelfAttention(\n",
      "        (Wq): Linear(in_features=8, out_features=8, bias=False)\n",
      "        (Wk): Linear(in_features=8, out_features=4, bias=False)\n",
      "        (Wv): Linear(in_features=8, out_features=4, bias=False)\n",
      "        (Wo): Linear(in_features=8, out_features=8, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wup): Linear(in_features=8, out_features=21, bias=False)\n",
      "        (Wgate): Linear(in_features=8, out_features=21, bias=False)\n",
      "        (Wdown): Linear(in_features=21, out_features=8, bias=False)\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (output): Linear(in_features=8, out_features=2048, bias=False)\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "\n",
      "====================Entering Model.forward====================\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([2, 1]), dtype: torch.int64, device: mps:0, min/max: 260.000/443.000\n",
      "Integer 'cache_len': Value=5\n",
      "List 'kv_cache':\n",
      "    Dict 'kv_cache[0]':\n",
      "        Tensor 'kv_cache[0][k]' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "        Tensor 'kv_cache[0][v]' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "Other-type 'target_token_ids': Type=NoneType, Value=None\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([2, 1, 2048]), dtype: torch.float32, device: mps:0, min/max: -0.215/0.201\n",
      "List 'output[1]':\n",
      "    Dict 'output[1][0]':\n",
      "        Tensor 'output[1][0][k]' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.028/0.146\n",
      "        Tensor 'output[1][0][v]' shape: torch.Size([2, 16, 1, 4]), dtype: torch.float32, device: mps:0, min/max: -0.073/0.077\n",
      "====================Exiting Model.forward====================\n",
      "CPU times: user 68.7 ms, sys: 9.72 ms, total: 78.4 ms\n",
      "Wall time: 86.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Inference\n",
    "module = Model(cfg).to(cfg.device)\n",
    "print(\"number of parameters: %.2fM\" % (module.get_num_params()/1e6,))\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "### enabling printing for sub-modules\n",
    "#for i in range(cfg.num_layers):\n",
    "    #module.layers[i].enable_logging()\n",
    "#module.final_norm.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "input_token_ids = torch.randint(tokenizer.vocab_len, (tcfg.micro_batch_size, 1)).to(cfg.device)\n",
    "kv_cache = [{ # Initialize kv caches for each layer\n",
    "                \"k\": torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device),\n",
    "                \"v\": torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device),\n",
    "            } for _ in range(cfg.num_layers)]\n",
    "\n",
    "output, kv_cache = module(input_token_ids, cache_len = cfg.max_seq_len // 3, kv_cache = kv_cache)\n",
    "\n",
    "del module, input_token_ids, kv_cache, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18f272a-9e60-4d41-9fe4-01a97099a4c9",
   "metadata": {},
   "source": [
    "# Other fun stuff\n",
    "\n",
    "### Learning Rate Schedule Display\n",
    "thought i'd make somewhere to help you visualize what the learning rate schedule settings you've got look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08801c2c-6f0f-4f41-a4de-25d8b10e2ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from train import scheduler_lambda\n",
    "from config import TrainConfig\n",
    "tcfg = TrainConfig()\n",
    "    \n",
    "# Generate learning rate values\n",
    "lrs = [scheduler_lambda(i) for i in range(tcfg.max_iters)]\n",
    "    \n",
    "# Plot the learning rates\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(lrs, label='Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b314d-4538-4192-b063-356bd3407fe5",
   "metadata": {},
   "source": [
    "### visualizing our tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d86faad0-346f-4ebb-927a-a093b217f5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Once',\n",
       " ' upon',\n",
       " ' a',\n",
       " ' time',\n",
       " ',',\n",
       " ' there',\n",
       " ' was',\n",
       " ' a',\n",
       " ' boy',\n",
       " ' named',\n",
       " ' Tim',\n",
       " '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tools import import_from_nested_path\n",
    "imported_objects = import_from_nested_path(['custom_tokenizers', cfg.tokenizer], 'tokenizer', ['get_tokenizer'])\n",
    "get_tokenizer = imported_objects.get('get_tokenizer')\n",
    "tokenizer = get_tokenizer(size = cfg.vocab_len)\n",
    "prompt = 'Once upon a time, there was a boy named Tim.'\n",
    "tokenizer.display(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de04bc7f-a633-4bc6-8189-364a1a643844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
