{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea126987-59aa-4f76-b926-6d632887c30b",
   "metadata": {},
   "source": [
    "# This notebook is designed for teaching/testing purposes to help you visualize the tensor shapes that go through each module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f833b1f8-ea91-4ae5-b3a3-73e08e4c8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my virtual environments are rarely properly connected to jupyter so this fixes that. \n",
    "# you prolly won't need this cell but running it won't hurt anything either\n",
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()  # Get the current working directory\n",
    "venv_dir = os.path.join(current_dir, 'venv') \n",
    "python_version = str(sys.version_info.major) + '.' + str(sys.version_info.minor)\n",
    "site_packages_path = os.path.join(venv_dir, 'lib', 'python' + python_version, 'site-packages')\n",
    "sys.path.append(site_packages_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c2c04f-2dbd-4020-8d91-cc0e4e8511b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelConfig(dim=96, device='mps', linear_bias=False, out_weight_share=True, pos_enc_type='RoPE', theta=10000, tokenizer='bpe_tinyStories', vocab_len=2048, num_layers=8, second_resid_norm=False, mlp_hidden_mult=4, mlp_nonlinearity='SiLU', mlp_gated=True, num_q_heads=2, num_kv_heads=1, head_dim=48, max_seq_len=512, scale_first_resid=True, norm_type='RMSNorm', norm_affine=True, norm_bias=True, eps=1e-06)\n",
      "TrainConfig(model_name='2024-07-01|23-45-54', dataset_name='noanabeshima/TinyStoriesV2', data_subset=None, streaming=False, micro_batch_size=8, grad_accum_steps=8, max_iters=10, eval_interval=2, eval_samples=1, checkpoint_interval=None, beta1=0.9, beta2=0.95, epsilon=1e-08, weight_decay=0.05, grad_clip=1.0, lr_init=0.0001, lr_max=0.1, lr_min=0.01, warmup_iters=1, final_flat_iters=0, anneal_type='cos', num_restarts=0, T_mult=2)\n"
     ]
    }
   ],
   "source": [
    "# config file\n",
    "from config import ModelConfig, TrainConfig\n",
    "cfg = ModelConfig()\n",
    "tcfg = TrainConfig()\n",
    "print(cfg)\n",
    "print(tcfg)\n",
    "\n",
    "# import the tokenizer specified by cfg\n",
    "from tools import import_from_nested_path\n",
    "imported_objects = import_from_nested_path(['custom_tokenizers', cfg.tokenizer], 'tokenizer', ['get_tokenizer'])\n",
    "get_tokenizer = imported_objects.get('get_tokenizer')\n",
    "tokenizer = get_tokenizer(size = 512) # assuming 'bpe', size options are 512, 1024 and 2048\n",
    "\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c12e3b-dc63-4479-ad55-b05d96364d1f",
   "metadata": {},
   "source": [
    "# Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3debc7b1-a7ec-4fb3-98cb-d16edf7c71cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.norm import Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "627969e9-9017-43f3-90ec-9a485abef26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.19K\n",
      "Norm()\n",
      "\n",
      "====================Entering Norm.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -4.438/4.463\n",
      "\n",
      "====================Entering Norm.RMSNorm====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -4.438/4.463\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -4.107/4.442\n",
      "====================Exiting Norm.RMSNorm====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -4.107/4.442\n",
      "====================Exiting Norm.forward====================\n",
      "CPU times: user 70.3 ms, sys: 54 ms, total: 124 ms\n",
      "Wall time: 155 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "### RMSNorm\n",
    "\n",
    "# Create an instance of RMSNorm\n",
    "module = Norm(cfg.dim, 'RMSNorm').to(cfg.device)\n",
    "\n",
    "# let's take a look\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "\n",
    "# Initially, logging is disabled by default\n",
    "module.enable_logging()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('CosineNorm')\n",
    "#module.disable_function_logging('LayerNorm')\n",
    "#module.disable_function_logging('RMSNorm')\n",
    "\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "output = module(x)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64020a96-205a-45b7-ae56-a3555d2b3719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================Entering Norm.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -4.717/4.219\n",
      "Full tensor content:\n",
      "tensor([[[-3.9557e-01,  7.7494e-02, -2.5963e-01,  ...,  1.6729e+00,\n",
      "          -4.1356e-01, -1.6519e+00],\n",
      "         [ 2.7144e-01,  3.3473e-01,  7.4003e-01,  ..., -1.1379e+00,\n",
      "           6.1507e-01,  8.3002e-01],\n",
      "         [-1.1509e+00, -6.2294e-01, -2.8028e-02,  ...,  1.6319e+00,\n",
      "          -8.4013e-01,  7.8838e-01],\n",
      "         ...,\n",
      "         [ 2.3112e-01, -2.0785e+00, -1.8716e-01,  ..., -4.1410e-01,\n",
      "          -2.6546e-01,  5.2926e-01],\n",
      "         [-3.2608e-01, -6.3929e-01,  5.6020e-01,  ..., -9.2344e-02,\n",
      "           1.7099e+00, -1.1762e+00],\n",
      "         [-7.8820e-04,  5.4825e-01, -1.1376e+00,  ..., -6.5838e-01,\n",
      "          -1.3714e+00, -1.6825e-01]],\n",
      "\n",
      "        [[ 4.9750e-01,  7.8290e-01,  6.6976e-01,  ...,  5.9991e-01,\n",
      "           2.8689e-01, -2.1704e-01],\n",
      "         [ 1.9792e-01,  1.9939e+00, -2.1671e+00,  ...,  1.2732e+00,\n",
      "           4.1384e-01,  2.0438e-02],\n",
      "         [-2.0663e+00, -2.8462e-01,  2.6054e-01,  ..., -1.0515e+00,\n",
      "          -2.4979e-01,  9.1613e-01],\n",
      "         ...,\n",
      "         [-5.2235e-01, -2.9162e-01, -6.4822e-01,  ..., -7.6420e-01,\n",
      "           3.4529e-01,  8.8795e-01],\n",
      "         [-6.5929e-01,  4.1442e-02,  1.2884e-01,  ...,  7.4746e-01,\n",
      "          -8.9535e-01, -1.9251e+00],\n",
      "         [ 5.5963e-01,  2.7875e-01,  1.2218e+00,  ..., -9.3884e-01,\n",
      "          -7.0069e-01,  5.7657e-01]],\n",
      "\n",
      "        [[ 2.3056e+00,  3.1193e+00,  9.6552e-01,  ...,  7.2345e-01,\n",
      "          -2.1779e-01, -3.3387e-01],\n",
      "         [ 1.8300e-01, -1.5211e+00,  1.2338e+00,  ..., -2.2788e-01,\n",
      "          -1.6191e+00,  7.8918e-02],\n",
      "         [ 9.4049e-02,  2.6222e-01, -8.3967e-01,  ...,  1.0694e+00,\n",
      "           1.1728e+00, -1.4261e+00],\n",
      "         ...,\n",
      "         [ 5.7751e-01, -7.3232e-01, -4.9255e-01,  ..., -1.7893e-01,\n",
      "          -2.9640e-01,  1.5503e+00],\n",
      "         [-7.8578e-02, -1.1118e+00,  1.2128e+00,  ..., -7.1378e-02,\n",
      "           6.9430e-01, -1.1850e-01],\n",
      "         [-7.8998e-01, -6.5895e-02,  1.6796e+00,  ..., -6.2203e-01,\n",
      "          -1.0216e-01,  4.3125e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.3724e-01,  1.2966e+00, -1.3244e+00,  ...,  1.7355e+00,\n",
      "          -2.3816e-01,  3.4426e-01],\n",
      "         [ 3.4584e-02, -1.3825e-01, -1.3352e+00,  ...,  1.5315e+00,\n",
      "          -1.0913e+00, -1.6712e-01],\n",
      "         [-1.8588e-01,  1.6471e+00, -4.4200e-01,  ..., -1.2614e+00,\n",
      "           5.8884e-01, -1.5205e+00],\n",
      "         ...,\n",
      "         [-5.3032e-01,  5.4570e-01, -4.9019e-01,  ...,  1.4494e+00,\n",
      "          -3.1436e-01, -9.4749e-01],\n",
      "         [ 2.3221e-01, -4.7729e-01,  6.6566e-01,  ...,  6.3673e-01,\n",
      "          -4.1169e-01,  2.9741e+00],\n",
      "         [-2.2213e-02,  7.0452e-01, -1.6011e+00,  ..., -1.1748e-02,\n",
      "           1.4974e+00,  5.8557e-01]],\n",
      "\n",
      "        [[ 2.5970e-01, -4.9965e-01,  1.0034e+00,  ..., -4.0707e-01,\n",
      "           6.5574e-01, -4.2787e-01],\n",
      "         [ 2.1276e+00, -1.6373e+00, -7.6606e-02,  ..., -7.3162e-01,\n",
      "          -1.5353e+00, -6.9315e-01],\n",
      "         [-1.3762e-01, -3.9437e-01,  1.6237e+00,  ..., -9.6326e-01,\n",
      "           2.4759e-01, -5.0269e-01],\n",
      "         ...,\n",
      "         [-1.2508e+00, -5.1605e-01,  6.2456e-01,  ...,  1.5866e-01,\n",
      "           1.0824e+00,  1.6138e+00],\n",
      "         [ 1.8382e-01,  1.2696e+00,  9.4412e-01,  ..., -7.9925e-05,\n",
      "          -1.0682e+00,  6.1555e-01],\n",
      "         [ 4.4101e-02,  9.3929e-02,  9.5234e-02,  ...,  9.7131e-02,\n",
      "          -2.4075e-01, -3.3319e-04]],\n",
      "\n",
      "        [[-1.3965e+00,  1.0400e+00,  4.4399e-01,  ...,  1.4736e-01,\n",
      "           2.4847e-01,  1.3320e+00],\n",
      "         [-9.5692e-01, -5.6149e-01,  3.6701e-01,  ...,  2.1324e+00,\n",
      "           1.4699e+00,  4.2350e-01],\n",
      "         [ 3.8596e-01, -1.1088e+00, -5.3574e-01,  ...,  2.3136e+00,\n",
      "          -5.4417e-01, -3.7381e-01],\n",
      "         ...,\n",
      "         [ 4.1527e-01, -1.9201e+00,  1.1943e-01,  ..., -1.4337e+00,\n",
      "           3.8062e-01,  6.2665e-01],\n",
      "         [ 3.6058e-01, -9.3587e-01,  8.1942e-01,  ..., -1.6065e+00,\n",
      "          -1.1481e+00,  1.2681e+00],\n",
      "         [-5.8731e-01, -6.7696e-01,  2.1761e-01,  ..., -2.0318e+00,\n",
      "           8.1285e-01,  1.0510e+00]]], device='mps:0')\n",
      "\n",
      "====================Entering Norm.LayerNorm====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -4.717/4.219\n",
      "Full tensor content:\n",
      "tensor([[[-3.9557e-01,  7.7494e-02, -2.5963e-01,  ...,  1.6729e+00,\n",
      "          -4.1356e-01, -1.6519e+00],\n",
      "         [ 2.7144e-01,  3.3473e-01,  7.4003e-01,  ..., -1.1379e+00,\n",
      "           6.1507e-01,  8.3002e-01],\n",
      "         [-1.1509e+00, -6.2294e-01, -2.8028e-02,  ...,  1.6319e+00,\n",
      "          -8.4013e-01,  7.8838e-01],\n",
      "         ...,\n",
      "         [ 2.3112e-01, -2.0785e+00, -1.8716e-01,  ..., -4.1410e-01,\n",
      "          -2.6546e-01,  5.2926e-01],\n",
      "         [-3.2608e-01, -6.3929e-01,  5.6020e-01,  ..., -9.2344e-02,\n",
      "           1.7099e+00, -1.1762e+00],\n",
      "         [-7.8820e-04,  5.4825e-01, -1.1376e+00,  ..., -6.5838e-01,\n",
      "          -1.3714e+00, -1.6825e-01]],\n",
      "\n",
      "        [[ 4.9750e-01,  7.8290e-01,  6.6976e-01,  ...,  5.9991e-01,\n",
      "           2.8689e-01, -2.1704e-01],\n",
      "         [ 1.9792e-01,  1.9939e+00, -2.1671e+00,  ...,  1.2732e+00,\n",
      "           4.1384e-01,  2.0438e-02],\n",
      "         [-2.0663e+00, -2.8462e-01,  2.6054e-01,  ..., -1.0515e+00,\n",
      "          -2.4979e-01,  9.1613e-01],\n",
      "         ...,\n",
      "         [-5.2235e-01, -2.9162e-01, -6.4822e-01,  ..., -7.6420e-01,\n",
      "           3.4529e-01,  8.8795e-01],\n",
      "         [-6.5929e-01,  4.1442e-02,  1.2884e-01,  ...,  7.4746e-01,\n",
      "          -8.9535e-01, -1.9251e+00],\n",
      "         [ 5.5963e-01,  2.7875e-01,  1.2218e+00,  ..., -9.3884e-01,\n",
      "          -7.0069e-01,  5.7657e-01]],\n",
      "\n",
      "        [[ 2.3056e+00,  3.1193e+00,  9.6552e-01,  ...,  7.2345e-01,\n",
      "          -2.1779e-01, -3.3387e-01],\n",
      "         [ 1.8300e-01, -1.5211e+00,  1.2338e+00,  ..., -2.2788e-01,\n",
      "          -1.6191e+00,  7.8918e-02],\n",
      "         [ 9.4049e-02,  2.6222e-01, -8.3967e-01,  ...,  1.0694e+00,\n",
      "           1.1728e+00, -1.4261e+00],\n",
      "         ...,\n",
      "         [ 5.7751e-01, -7.3232e-01, -4.9255e-01,  ..., -1.7893e-01,\n",
      "          -2.9640e-01,  1.5503e+00],\n",
      "         [-7.8578e-02, -1.1118e+00,  1.2128e+00,  ..., -7.1378e-02,\n",
      "           6.9430e-01, -1.1850e-01],\n",
      "         [-7.8998e-01, -6.5895e-02,  1.6796e+00,  ..., -6.2203e-01,\n",
      "          -1.0216e-01,  4.3125e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.3724e-01,  1.2966e+00, -1.3244e+00,  ...,  1.7355e+00,\n",
      "          -2.3816e-01,  3.4426e-01],\n",
      "         [ 3.4584e-02, -1.3825e-01, -1.3352e+00,  ...,  1.5315e+00,\n",
      "          -1.0913e+00, -1.6712e-01],\n",
      "         [-1.8588e-01,  1.6471e+00, -4.4200e-01,  ..., -1.2614e+00,\n",
      "           5.8884e-01, -1.5205e+00],\n",
      "         ...,\n",
      "         [-5.3032e-01,  5.4570e-01, -4.9019e-01,  ...,  1.4494e+00,\n",
      "          -3.1436e-01, -9.4749e-01],\n",
      "         [ 2.3221e-01, -4.7729e-01,  6.6566e-01,  ...,  6.3673e-01,\n",
      "          -4.1169e-01,  2.9741e+00],\n",
      "         [-2.2213e-02,  7.0452e-01, -1.6011e+00,  ..., -1.1748e-02,\n",
      "           1.4974e+00,  5.8557e-01]],\n",
      "\n",
      "        [[ 2.5970e-01, -4.9965e-01,  1.0034e+00,  ..., -4.0707e-01,\n",
      "           6.5574e-01, -4.2787e-01],\n",
      "         [ 2.1276e+00, -1.6373e+00, -7.6606e-02,  ..., -7.3162e-01,\n",
      "          -1.5353e+00, -6.9315e-01],\n",
      "         [-1.3762e-01, -3.9437e-01,  1.6237e+00,  ..., -9.6326e-01,\n",
      "           2.4759e-01, -5.0269e-01],\n",
      "         ...,\n",
      "         [-1.2508e+00, -5.1605e-01,  6.2456e-01,  ...,  1.5866e-01,\n",
      "           1.0824e+00,  1.6138e+00],\n",
      "         [ 1.8382e-01,  1.2696e+00,  9.4412e-01,  ..., -7.9925e-05,\n",
      "          -1.0682e+00,  6.1555e-01],\n",
      "         [ 4.4101e-02,  9.3929e-02,  9.5234e-02,  ...,  9.7131e-02,\n",
      "          -2.4075e-01, -3.3319e-04]],\n",
      "\n",
      "        [[-1.3965e+00,  1.0400e+00,  4.4399e-01,  ...,  1.4736e-01,\n",
      "           2.4847e-01,  1.3320e+00],\n",
      "         [-9.5692e-01, -5.6149e-01,  3.6701e-01,  ...,  2.1324e+00,\n",
      "           1.4699e+00,  4.2350e-01],\n",
      "         [ 3.8596e-01, -1.1088e+00, -5.3574e-01,  ...,  2.3136e+00,\n",
      "          -5.4417e-01, -3.7381e-01],\n",
      "         ...,\n",
      "         [ 4.1527e-01, -1.9201e+00,  1.1943e-01,  ..., -1.4337e+00,\n",
      "           3.8062e-01,  6.2665e-01],\n",
      "         [ 3.6058e-01, -9.3587e-01,  8.1942e-01,  ..., -1.6065e+00,\n",
      "          -1.1481e+00,  1.2681e+00],\n",
      "         [-5.8731e-01, -6.7696e-01,  2.1761e-01,  ..., -2.0318e+00,\n",
      "           8.1285e-01,  1.0510e+00]]], device='mps:0')\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -4.807/4.321\n",
      "Full tensor content:\n",
      "tensor([[[-2.4162e-01,  2.0037e-01, -1.1461e-01,  ...,  1.6910e+00,\n",
      "          -2.5843e-01, -1.4155e+00],\n",
      "         [ 2.8700e-01,  3.5144e-01,  7.6412e-01,  ..., -1.1480e+00,\n",
      "           6.3690e-01,  8.5576e-01],\n",
      "         [-1.2461e+00, -6.5964e-01,  1.1706e-03,  ...,  1.8450e+00,\n",
      "          -9.0089e-01,  9.0801e-01],\n",
      "         ...,\n",
      "         [ 3.5064e-01, -1.8869e+00, -5.4600e-02,  ..., -2.7445e-01,\n",
      "          -1.3045e-01,  6.3947e-01],\n",
      "         [-2.8173e-01, -5.8867e-01,  5.8679e-01,  ..., -5.2678e-02,\n",
      "           1.7135e+00, -1.1148e+00],\n",
      "         [-1.1672e-01,  4.0223e-01, -1.1913e+00,  ..., -7.3829e-01,\n",
      "          -1.4122e+00, -2.7501e-01]],\n",
      "\n",
      "        [[ 6.6026e-01,  9.4452e-01,  8.3184e-01,  ...,  7.6226e-01,\n",
      "           4.5049e-01, -5.1434e-02],\n",
      "         [ 1.2025e-01,  1.7506e+00, -2.0268e+00,  ...,  1.0964e+00,\n",
      "           3.1627e-01, -4.0871e-02],\n",
      "         [-2.3414e+00, -3.7080e-01,  2.3215e-01,  ..., -1.2190e+00,\n",
      "          -3.3229e-01,  9.5724e-01],\n",
      "         ...,\n",
      "         [-4.7898e-01, -2.0750e-01, -6.2708e-01,  ..., -7.6354e-01,\n",
      "           5.4189e-01,  1.1804e+00],\n",
      "         [-5.2906e-01,  1.7205e-01,  2.5949e-01,  ...,  8.7845e-01,\n",
      "          -7.6524e-01, -1.7955e+00],\n",
      "         [ 4.9582e-01,  2.3630e-01,  1.1076e+00,  ..., -8.8867e-01,\n",
      "          -6.6863e-01,  5.1148e-01]],\n",
      "\n",
      "        [[ 2.0021e+00,  2.7832e+00,  7.1563e-01,  ...,  4.8324e-01,\n",
      "          -4.2031e-01, -5.3175e-01],\n",
      "         [ 2.7225e-01, -1.3252e+00,  1.2573e+00,  ..., -1.1290e-01,\n",
      "          -1.4171e+00,  1.7469e-01],\n",
      "         [ 4.5474e-02,  2.2092e-01, -9.2860e-01,  ...,  1.0629e+00,\n",
      "           1.1708e+00, -1.5404e+00],\n",
      "         ...,\n",
      "         [ 5.4009e-01, -8.3278e-01, -5.8147e-01,  ..., -2.5276e-01,\n",
      "          -3.7588e-01,  1.5596e+00],\n",
      "         [ 7.2978e-02, -9.1801e-01,  1.3116e+00,  ...,  7.9884e-02,\n",
      "           8.1428e-01,  3.4688e-02],\n",
      "         [-9.7727e-01, -1.4864e-01,  1.8489e+00,  ..., -7.8508e-01,\n",
      "          -1.9014e-01,  4.2029e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.9675e-01,  1.0858e+00, -1.2767e+00,  ...,  1.4814e+00,\n",
      "          -2.9758e-01,  2.2739e-01],\n",
      "         [ 2.9579e-02, -1.4429e-01, -1.3484e+00,  ...,  1.5355e+00,\n",
      "          -1.1031e+00, -1.7334e-01],\n",
      "         [-2.1642e-01,  1.4650e+00, -4.5136e-01,  ..., -1.2030e+00,\n",
      "           4.9422e-01, -1.4407e+00],\n",
      "         ...,\n",
      "         [-6.2037e-01,  4.6553e-01, -5.7987e-01,  ...,  1.3775e+00,\n",
      "          -4.0243e-01, -1.0414e+00],\n",
      "         [ 2.1564e-01, -4.3256e-01,  6.1164e-01,  ...,  5.8522e-01,\n",
      "          -3.7263e-01,  2.7207e+00],\n",
      "         [-3.4372e-02,  6.6619e-01, -1.5564e+00,  ..., -2.4283e-02,\n",
      "           1.4305e+00,  5.5153e-01]],\n",
      "\n",
      "        [[ 2.6051e-01, -4.5373e-01,  9.5998e-01,  ..., -3.6665e-01,\n",
      "           6.3302e-01, -3.8622e-01],\n",
      "         [ 2.1612e+00, -1.5914e+00, -3.5788e-02,  ..., -6.8865e-01,\n",
      "          -1.4897e+00, -6.5030e-01],\n",
      "         [-1.6716e-01, -4.5276e-01,  1.7921e+00,  ..., -1.0856e+00,\n",
      "           2.6135e-01, -5.7326e-01],\n",
      "         ...,\n",
      "         [-1.4731e+00, -5.9974e-01,  7.5597e-01,  ...,  2.0221e-01,\n",
      "           1.3002e+00,  1.9317e+00],\n",
      "         [ 1.0349e-01,  1.3209e+00,  9.5601e-01,  ..., -1.0271e-01,\n",
      "          -1.3004e+00,  5.8758e-01],\n",
      "         [-1.2335e-01, -7.0990e-02, -6.9619e-02,  ..., -6.7625e-02,\n",
      "          -4.2265e-01, -1.7003e-01]],\n",
      "\n",
      "        [[-1.5066e+00,  9.5927e-01,  3.5605e-01,  ...,  5.5844e-02,\n",
      "           1.5817e-01,  1.2548e+00],\n",
      "         [-1.2413e+00, -7.9020e-01,  2.6901e-01,  ...,  2.2829e+00,\n",
      "           1.5272e+00,  3.3345e-01],\n",
      "         [ 5.7773e-01, -1.2272e+00, -5.3524e-01,  ...,  2.9054e+00,\n",
      "          -5.4543e-01, -3.3971e-01],\n",
      "         ...,\n",
      "         [ 4.0673e-01, -1.9775e+00,  1.0470e-01,  ..., -1.4809e+00,\n",
      "           3.7135e-01,  6.2253e-01],\n",
      "         [ 4.0580e-01, -8.5584e-01,  8.5232e-01,  ..., -1.5084e+00,\n",
      "          -1.0623e+00,  1.2889e+00],\n",
      "         [-5.0426e-01, -5.8645e-01,  2.3371e-01,  ..., -1.8286e+00,\n",
      "           7.7943e-01,  9.9774e-01]]], device='mps:0')\n",
      "====================Exiting Norm.LayerNorm====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -4.807/4.321\n",
      "Full tensor content:\n",
      "tensor([[[-2.4162e-01,  2.0037e-01, -1.1461e-01,  ...,  1.6910e+00,\n",
      "          -2.5843e-01, -1.4155e+00],\n",
      "         [ 2.8700e-01,  3.5144e-01,  7.6412e-01,  ..., -1.1480e+00,\n",
      "           6.3690e-01,  8.5576e-01],\n",
      "         [-1.2461e+00, -6.5964e-01,  1.1706e-03,  ...,  1.8450e+00,\n",
      "          -9.0089e-01,  9.0801e-01],\n",
      "         ...,\n",
      "         [ 3.5064e-01, -1.8869e+00, -5.4600e-02,  ..., -2.7445e-01,\n",
      "          -1.3045e-01,  6.3947e-01],\n",
      "         [-2.8173e-01, -5.8867e-01,  5.8679e-01,  ..., -5.2678e-02,\n",
      "           1.7135e+00, -1.1148e+00],\n",
      "         [-1.1672e-01,  4.0223e-01, -1.1913e+00,  ..., -7.3829e-01,\n",
      "          -1.4122e+00, -2.7501e-01]],\n",
      "\n",
      "        [[ 6.6026e-01,  9.4452e-01,  8.3184e-01,  ...,  7.6226e-01,\n",
      "           4.5049e-01, -5.1434e-02],\n",
      "         [ 1.2025e-01,  1.7506e+00, -2.0268e+00,  ...,  1.0964e+00,\n",
      "           3.1627e-01, -4.0871e-02],\n",
      "         [-2.3414e+00, -3.7080e-01,  2.3215e-01,  ..., -1.2190e+00,\n",
      "          -3.3229e-01,  9.5724e-01],\n",
      "         ...,\n",
      "         [-4.7898e-01, -2.0750e-01, -6.2708e-01,  ..., -7.6354e-01,\n",
      "           5.4189e-01,  1.1804e+00],\n",
      "         [-5.2906e-01,  1.7205e-01,  2.5949e-01,  ...,  8.7845e-01,\n",
      "          -7.6524e-01, -1.7955e+00],\n",
      "         [ 4.9582e-01,  2.3630e-01,  1.1076e+00,  ..., -8.8867e-01,\n",
      "          -6.6863e-01,  5.1148e-01]],\n",
      "\n",
      "        [[ 2.0021e+00,  2.7832e+00,  7.1563e-01,  ...,  4.8324e-01,\n",
      "          -4.2031e-01, -5.3175e-01],\n",
      "         [ 2.7225e-01, -1.3252e+00,  1.2573e+00,  ..., -1.1290e-01,\n",
      "          -1.4171e+00,  1.7469e-01],\n",
      "         [ 4.5474e-02,  2.2092e-01, -9.2860e-01,  ...,  1.0629e+00,\n",
      "           1.1708e+00, -1.5404e+00],\n",
      "         ...,\n",
      "         [ 5.4009e-01, -8.3278e-01, -5.8147e-01,  ..., -2.5276e-01,\n",
      "          -3.7588e-01,  1.5596e+00],\n",
      "         [ 7.2978e-02, -9.1801e-01,  1.3116e+00,  ...,  7.9884e-02,\n",
      "           8.1428e-01,  3.4688e-02],\n",
      "         [-9.7727e-01, -1.4864e-01,  1.8489e+00,  ..., -7.8508e-01,\n",
      "          -1.9014e-01,  4.2029e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.9675e-01,  1.0858e+00, -1.2767e+00,  ...,  1.4814e+00,\n",
      "          -2.9758e-01,  2.2739e-01],\n",
      "         [ 2.9579e-02, -1.4429e-01, -1.3484e+00,  ...,  1.5355e+00,\n",
      "          -1.1031e+00, -1.7334e-01],\n",
      "         [-2.1642e-01,  1.4650e+00, -4.5136e-01,  ..., -1.2030e+00,\n",
      "           4.9422e-01, -1.4407e+00],\n",
      "         ...,\n",
      "         [-6.2037e-01,  4.6553e-01, -5.7987e-01,  ...,  1.3775e+00,\n",
      "          -4.0243e-01, -1.0414e+00],\n",
      "         [ 2.1564e-01, -4.3256e-01,  6.1164e-01,  ...,  5.8522e-01,\n",
      "          -3.7263e-01,  2.7207e+00],\n",
      "         [-3.4372e-02,  6.6619e-01, -1.5564e+00,  ..., -2.4283e-02,\n",
      "           1.4305e+00,  5.5153e-01]],\n",
      "\n",
      "        [[ 2.6051e-01, -4.5373e-01,  9.5998e-01,  ..., -3.6665e-01,\n",
      "           6.3302e-01, -3.8622e-01],\n",
      "         [ 2.1612e+00, -1.5914e+00, -3.5788e-02,  ..., -6.8865e-01,\n",
      "          -1.4897e+00, -6.5030e-01],\n",
      "         [-1.6716e-01, -4.5276e-01,  1.7921e+00,  ..., -1.0856e+00,\n",
      "           2.6135e-01, -5.7326e-01],\n",
      "         ...,\n",
      "         [-1.4731e+00, -5.9974e-01,  7.5597e-01,  ...,  2.0221e-01,\n",
      "           1.3002e+00,  1.9317e+00],\n",
      "         [ 1.0349e-01,  1.3209e+00,  9.5601e-01,  ..., -1.0271e-01,\n",
      "          -1.3004e+00,  5.8758e-01],\n",
      "         [-1.2335e-01, -7.0990e-02, -6.9619e-02,  ..., -6.7625e-02,\n",
      "          -4.2265e-01, -1.7003e-01]],\n",
      "\n",
      "        [[-1.5066e+00,  9.5927e-01,  3.5605e-01,  ...,  5.5844e-02,\n",
      "           1.5817e-01,  1.2548e+00],\n",
      "         [-1.2413e+00, -7.9020e-01,  2.6901e-01,  ...,  2.2829e+00,\n",
      "           1.5272e+00,  3.3345e-01],\n",
      "         [ 5.7773e-01, -1.2272e+00, -5.3524e-01,  ...,  2.9054e+00,\n",
      "          -5.4543e-01, -3.3971e-01],\n",
      "         ...,\n",
      "         [ 4.0673e-01, -1.9775e+00,  1.0470e-01,  ..., -1.4809e+00,\n",
      "           3.7135e-01,  6.2253e-01],\n",
      "         [ 4.0580e-01, -8.5584e-01,  8.5232e-01,  ..., -1.5084e+00,\n",
      "          -1.0623e+00,  1.2889e+00],\n",
      "         [-5.0426e-01, -5.8645e-01,  2.3371e-01,  ..., -1.8286e+00,\n",
      "           7.7943e-01,  9.9774e-01]]], device='mps:0', grad_fn=<AddBackward0>)\n",
      "====================Exiting Norm.forward====================\n",
      "CPU times: user 143 ms, sys: 79.6 ms, total: 223 ms\n",
      "Wall time: 569 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# LayerNorm\n",
    "module = Norm(cfg.dim, 'LayerNorm').to(cfg.device)\n",
    "module.enable_logging()\n",
    "\n",
    "# you can also have it optionally print out all tensors in full\n",
    "module.enable_full_tensor_printing()\n",
    "# i recommend only doing this with very small toy values for your hyperparameters, otherwise this gets too big\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('CosineNorm')\n",
    "#module.disable_function_logging('LayerNorm')\n",
    "#module.disable_function_logging('RMSNorm')\n",
    "\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "output = module(x)\n",
    "module.disable_logging()\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e190f3de-37fd-442b-bfb1-6a090115fc75",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a89e1e8f-cedd-4885-ad50-934827ed045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.attention import SelfAttention, PrecomputeRotaryFrequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e8f27b4-6d1f-4fcd-99b0-2284e65d6b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 27.65K\n",
      "SelfAttention(\n",
      "  (Wq): Linear(in_features=96, out_features=96, bias=False)\n",
      "  (Wk): Linear(in_features=96, out_features=48, bias=False)\n",
      "  (Wv): Linear(in_features=96, out_features=48, bias=False)\n",
      "  (Wo): Linear(in_features=96, out_features=96, bias=False)\n",
      ")\n",
      "\n",
      "====================Entering SelfAttention.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -4.616/4.731\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([512, 512]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "====================Entering SelfAttention.apply_rotary_pos_emb====================\n",
      "Inputs:\n",
      "Tensor 'q' shape: torch.Size([8, 512, 2, 48]), dtype: torch.float32, device: mps:0, min/max: -2.911/2.686\n",
      "Tensor 'k' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -2.503/2.450\n",
      "Dict 'freqs_cis':\n",
      "    Tensor 'freqs_cis[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs_cis[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "\n",
      "====================Entering SelfAttention.rotate_half====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 2, 48]), dtype: torch.float32, device: mps:0, min/max: -2.911/2.686\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 512, 2, 48]), dtype: torch.float32, device: mps:0, min/max: -2.911/2.686\n",
      "====================Exiting SelfAttention.rotate_half====================\n",
      "\n",
      "====================Entering SelfAttention.rotate_half====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -2.503/2.450\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -2.459/2.503\n",
      "====================Exiting SelfAttention.rotate_half====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 2, 48]), dtype: torch.float32, device: mps:0, min/max: -2.867/2.629\n",
      "Tensor 'output[1]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -2.560/2.516\n",
      "====================Exiting SelfAttention.apply_rotary_pos_emb====================\n",
      "\n",
      "====================Entering SelfAttention.match_headcount====================\n",
      "Inputs:\n",
      "Tensor 'k' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -2.560/2.516\n",
      "Tensor 'v' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -2.766/2.788\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 2, 48]), dtype: torch.float32, device: mps:0, min/max: -2.560/2.516\n",
      "Tensor 'output[1]' shape: torch.Size([8, 512, 2, 48]), dtype: torch.float32, device: mps:0, min/max: -2.766/2.788\n",
      "====================Exiting SelfAttention.match_headcount====================\n",
      "\n",
      "====================Entering SelfAttention.flash_attention====================\n",
      "Inputs:\n",
      "Tensor 'q' shape: torch.Size([8, 2, 512, 48]), dtype: torch.float32, device: mps:0, min/max: -2.867/2.629\n",
      "Tensor 'k' shape: torch.Size([8, 2, 512, 48]), dtype: torch.float32, device: mps:0, min/max: -2.560/2.516\n",
      "Tensor 'v' shape: torch.Size([8, 2, 512, 48]), dtype: torch.float32, device: mps:0, min/max: -2.766/2.788\n",
      "Tensor 'mask' shape: torch.Size([512, 512]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 2, 512, 48]), dtype: torch.float32, device: mps:0, min/max: -1.765/1.949\n",
      "====================Exiting SelfAttention.flash_attention====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -1.551/1.343\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting SelfAttention.forward====================\n",
      "CPU times: user 222 ms, sys: 104 ms, total: 326 ms\n",
      "Wall time: 443 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# first up let's look at training\n",
    "\n",
    "# Create an instance of multi-head self-attention\n",
    "module = SelfAttention(cfg.dim, cfg.head_dim, cfg.num_q_heads, cfg.num_kv_heads, cfg.max_seq_len, cfg.linear_bias, device=cfg.device)\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "\n",
    "# Initially, logging is disabled by default\n",
    "module.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "### Optionally disabling printing for sub-functions\n",
    "#module.disable_function_logging('apply_precompute_freqs')\n",
    "#module.disable_function_logging('reshape_for_broadcast')\n",
    "#module.disable_function_logging('match_headcount')\n",
    "#module.disable_function_logging('attend')\n",
    "#module.disable_function_logging('calc_output')\n",
    "\n",
    "# precompute RoPE frequencies, causal mask, and dummy input data\n",
    "if cfg.pos_enc_type == 'RoPE':\n",
    "    precompute_freqs = PrecomputeRotaryFrequencies(cfg.head_dim, cfg.max_seq_len, cfg.theta, cfg.device)\n",
    "    freqs = precompute_freqs()\n",
    "else:\n",
    "    freqs = None\n",
    "mask = torch.ones(cfg.max_seq_len, cfg.max_seq_len, dtype=torch.bool, device=cfg.device).triu(diagonal=1)\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "output = module(x, freqs, mask, training=True)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, freqs, mask, x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e7e034c-c5a6-43ac-b6d2-bbb71f9965ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================Entering SelfAttention.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 128, 96]), dtype: torch.float32, device: mps:0, min/max: -4.549/4.476\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([128, 495]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Integer 'cache_len': Value=367\n",
      "Dict 'kv_cache':\n",
      "    Tensor 'kv_cache[k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "    Tensor 'kv_cache[v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "Bool 'training': Value=False\n",
      "\n",
      "====================Entering SelfAttention.apply_rotary_pos_emb====================\n",
      "Inputs:\n",
      "Tensor 'q' shape: torch.Size([8, 128, 2, 48]), dtype: torch.float32, device: mps:0, min/max: -2.484/2.587\n",
      "Tensor 'k' shape: torch.Size([8, 128, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -2.309/2.276\n",
      "Dict 'freqs_cis':\n",
      "    Tensor 'freqs_cis[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs_cis[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Integer 'cache_len': Value=367\n",
      "\n",
      "====================Entering SelfAttention.rotate_half====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 128, 2, 48]), dtype: torch.float32, device: mps:0, min/max: -2.484/2.587\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 128, 2, 48]), dtype: torch.float32, device: mps:0, min/max: -2.447/2.587\n",
      "====================Exiting SelfAttention.rotate_half====================\n",
      "\n",
      "====================Entering SelfAttention.rotate_half====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 128, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -2.309/2.276\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 128, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -2.282/2.309\n",
      "====================Exiting SelfAttention.rotate_half====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 128, 2, 48]), dtype: torch.float32, device: mps:0, min/max: -2.616/2.606\n",
      "Tensor 'output[1]' shape: torch.Size([8, 128, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -2.554/2.463\n",
      "====================Exiting SelfAttention.apply_rotary_pos_emb====================\n",
      "\n",
      "====================Entering SelfAttention.match_headcount====================\n",
      "Inputs:\n",
      "Tensor 'k' shape: torch.Size([8, 495, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -2.554/2.463\n",
      "Tensor 'v' shape: torch.Size([8, 495, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -2.404/2.371\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 495, 2, 48]), dtype: torch.float32, device: mps:0, min/max: -2.554/2.463\n",
      "Tensor 'output[1]' shape: torch.Size([8, 495, 2, 48]), dtype: torch.float32, device: mps:0, min/max: -2.404/2.371\n",
      "====================Exiting SelfAttention.match_headcount====================\n",
      "\n",
      "====================Entering SelfAttention.flash_attention====================\n",
      "Inputs:\n",
      "Tensor 'q' shape: torch.Size([8, 2, 128, 48]), dtype: torch.float32, device: mps:0, min/max: -2.616/2.606\n",
      "Tensor 'k' shape: torch.Size([8, 2, 495, 48]), dtype: torch.float32, device: mps:0, min/max: -2.554/2.463\n",
      "Tensor 'v' shape: torch.Size([8, 2, 495, 48]), dtype: torch.float32, device: mps:0, min/max: -2.404/2.371\n",
      "Tensor 'mask' shape: torch.Size([128, 495]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Bool 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 2, 128, 48]), dtype: torch.float32, device: mps:0, min/max: -0.062/0.056\n",
      "====================Exiting SelfAttention.flash_attention====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 128, 96]), dtype: torch.float32, device: mps:0, min/max: -0.031/0.045\n",
      "Dict 'output[1]':\n",
      "    Tensor 'output[1][k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -2.554/2.463\n",
      "    Tensor 'output[1][v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -2.404/2.371\n",
      "====================Exiting SelfAttention.forward====================\n",
      "CPU times: user 136 ms, sys: 23.8 ms, total: 160 ms\n",
      "Wall time: 492 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# now let's do it for inference\n",
    "\n",
    "module = SelfAttention(cfg.dim, cfg.head_dim, cfg.num_q_heads, cfg.num_kv_heads, cfg.max_seq_len, cfg.linear_bias, device=cfg.device)\n",
    "module.enable_logging()\n",
    "#module.disable_function_logging('apply_precompute_freqs')\n",
    "#module.disable_function_logging('reshape_for_broadcast')\n",
    "#module.disable_function_logging('match_headcount')\n",
    "#module.disable_function_logging('attend')\n",
    "#module.disable_function_logging('calc_output')\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "# precompute RoPE frequencies, causal mask, and dummy input data\n",
    "if cfg.pos_enc_type == 'RoPE':\n",
    "    precompute_freqs = PrecomputeRotaryFrequencies(cfg.head_dim, cfg.max_seq_len, cfg.theta, cfg.device)\n",
    "    freqs = precompute_freqs()\n",
    "else:\n",
    "    freqs = None\n",
    "mask = torch.ones(cfg.max_seq_len, cfg.max_seq_len, dtype=torch.bool, device=cfg.device).triu(diagonal=1)\n",
    "# setting up for kv caching\n",
    "context_chunk_len = cfg.max_seq_len // 4\n",
    "cache_len = random.randint(1, 3 * context_chunk_len)\n",
    "seq_len = cache_len + context_chunk_len\n",
    "kv_cache = {\n",
    "    'k': torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device),\n",
    "    'v': torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device)\n",
    "}\n",
    "# need to extend the mask with zeros for the cached values\n",
    "mask = torch.nn.functional.pad(mask[:context_chunk_len, :context_chunk_len], (cache_len, 0, 0, 0), value=False).bool()\n",
    "x = torch.randn(tcfg.micro_batch_size,context_chunk_len,cfg.dim).to(cfg.device)\n",
    "\n",
    "# Call the forward method - logging will occur\n",
    "output = module(x, freqs, mask, cache_len, kv_cache)\n",
    "\n",
    "# Disable logging. \n",
    "# This isn't actually necessary since we won't be using this object again but that's how you'd do it\n",
    "module.disable_logging()\n",
    "\n",
    "# clearing up ram jic we're training later\n",
    "del module, freqs, mask, cache_len, context_chunk_len, seq_len, kv_cache, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb308c5-b578-46f2-86ae-bfa6800be641",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7736b685-f941-4182-a5b7-4731cce706b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.mlp import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c62e49e9-2189-4269-968e-1df99469dc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 73.73K\n",
      "MLP(\n",
      "  (Wup): Linear(in_features=96, out_features=256, bias=False)\n",
      "  (Wgate): Linear(in_features=96, out_features=256, bias=False)\n",
      "  (Wdown): Linear(in_features=256, out_features=96, bias=False)\n",
      "  (nonlinearity): GELU(approximate='none')\n",
      ")\n",
      "\n",
      "====================Entering MLP.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -4.755/4.784\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.714/0.729\n",
      "====================Exiting MLP.forward====================\n",
      "CPU times: user 31.8 ms, sys: 3.94 ms, total: 35.7 ms\n",
      "Wall time: 38 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# GeGLU\n",
    "module = MLP(\n",
    "    cfg.dim, \n",
    "    int(cfg.dim * cfg.mlp_hidden_mult * 2/3), \n",
    "    cfg.dim, \n",
    "    'GeLU', \n",
    "    gated=True, \n",
    "    bias=cfg.linear_bias, \n",
    "    dropout_rate = 0.1\n",
    ").to(cfg.device)\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "module.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "output = module(x, training=True)\n",
    "module.disable_logging()\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0748fa3-3230-4dd7-a768-1256ea72e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 74.21K\n",
      "MLP(\n",
      "  (Wup): Linear(in_features=96, out_features=384, bias=True)\n",
      "  (Wdown): Linear(in_features=384, out_features=96, bias=True)\n",
      "  (nonlinearity): ReLU()\n",
      ")\n",
      "\n",
      "====================Entering MLP.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -4.470/5.160\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -1.321/1.106\n",
      "====================Exiting MLP.forward====================\n",
      "CPU times: user 33.1 ms, sys: 5.43 ms, total: 38.5 ms\n",
      "Wall time: 220 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# not gated, testing every other nonlinearity\n",
    "module = MLP(\n",
    "    cfg.dim, \n",
    "    cfg.dim * cfg.mlp_hidden_mult, \n",
    "    cfg.dim, \n",
    "    'ReLU', \n",
    "    gated=False, \n",
    "    bias=True, \n",
    "    dropout_rate = 0.1\n",
    ").to(cfg.device)\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "module.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "output = module(x, training=True)\n",
    "module.disable_logging()\n",
    "del module, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3a502f-4646-4a02-9412-372482af9fa0",
   "metadata": {},
   "source": [
    "# ResidualLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a21d708-af47-4f32-b111-08efedc584f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.layer import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c886661d-5a26-4787-93d1-c3c8c6b1c02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 101.76K\n",
      "Layer(\n",
      "  (pre_attn_norm): Norm()\n",
      "  (attn): SelfAttention(\n",
      "    (Wq): Linear(in_features=96, out_features=96, bias=False)\n",
      "    (Wk): Linear(in_features=96, out_features=48, bias=False)\n",
      "    (Wv): Linear(in_features=96, out_features=48, bias=False)\n",
      "    (Wo): Linear(in_features=96, out_features=96, bias=False)\n",
      "  )\n",
      "  (pre_mlp_norm): Norm()\n",
      "  (mlp): MLP(\n",
      "    (Wup): Linear(in_features=96, out_features=256, bias=False)\n",
      "    (Wgate): Linear(in_features=96, out_features=256, bias=False)\n",
      "    (Wdown): Linear(in_features=256, out_features=96, bias=False)\n",
      "    (nonlinearity): SiLU()\n",
      "  )\n",
      ")\n",
      "\n",
      "====================Entering Layer.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -4.543/4.487\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([512, 512]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "====================Entering Layer.attn_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -4.543/4.487\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([512, 512]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -1.215/1.518\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.attn_connect====================\n",
      "\n",
      "====================Entering Layer.mlp_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -4.521/4.517\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.539/0.507\n",
      "====================Exiting Layer.mlp_connect====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -4.613/4.312\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.forward====================\n",
      "CPU times: user 32.8 ms, sys: 6.87 ms, total: 39.7 ms\n",
      "Wall time: 54.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TRAINING\n",
    "module = Layer(cfg).to(cfg.device)\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "#module.disable_function_logging('attn_connect')\n",
    "#module.disable_function_logging('mlp_connect')\n",
    "### enabling printing for sub-modules\n",
    "#module.pre_attn_norm.enable_logging()\n",
    "#module.attn.enable_logging()\n",
    "#module.post_attn_norm.enable_logging()\n",
    "#module.pre_mlp_norm.enable_logging()\n",
    "#module.mlp.enable_logging()\n",
    "#module.post_mlp_norm.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "# precompute RoPE frequencies, causal mask, and dummy input data\n",
    "if cfg.pos_enc_type == 'RoPE':\n",
    "    precompute_freqs = PrecomputeRotaryFrequencies(cfg.head_dim, cfg.max_seq_len, cfg.theta, cfg.device)\n",
    "    freqs = precompute_freqs()\n",
    "else:\n",
    "    freqs = None\n",
    "mask = torch.ones(cfg.max_seq_len, cfg.max_seq_len, dtype=torch.bool, device=cfg.device).triu(diagonal=1)\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "x = torch.randn(tcfg.micro_batch_size,cfg.max_seq_len,cfg.dim).to(cfg.device)\n",
    "\n",
    "output = module(x, freqs, mask, training=True)\n",
    "module.disable_logging()\n",
    "del module,freqs, mask, x, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f534ab4-a150-42db-8c89-5e8d3d0c06a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 101.76K\n",
      "Layer(\n",
      "  (pre_attn_norm): Norm()\n",
      "  (attn): SelfAttention(\n",
      "    (Wq): Linear(in_features=96, out_features=96, bias=False)\n",
      "    (Wk): Linear(in_features=96, out_features=48, bias=False)\n",
      "    (Wv): Linear(in_features=96, out_features=48, bias=False)\n",
      "    (Wo): Linear(in_features=96, out_features=96, bias=False)\n",
      "  )\n",
      "  (pre_mlp_norm): Norm()\n",
      "  (mlp): MLP(\n",
      "    (Wup): Linear(in_features=96, out_features=256, bias=False)\n",
      "    (Wgate): Linear(in_features=96, out_features=256, bias=False)\n",
      "    (Wdown): Linear(in_features=256, out_features=96, bias=False)\n",
      "    (nonlinearity): SiLU()\n",
      "  )\n",
      ")\n",
      "\n",
      "====================Entering Layer.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 128, 96]), dtype: torch.float32, device: mps:0, min/max: -4.284/4.109\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([128, 298]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Integer 'cache_len': Value=170\n",
      "Dict 'kv_cache':\n",
      "    Tensor 'kv_cache[k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "    Tensor 'kv_cache[v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "Bool 'training': Value=False\n",
      "\n",
      "====================Entering Layer.attn_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 128, 96]), dtype: torch.float32, device: mps:0, min/max: -4.284/4.109\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([128, 298]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Integer 'cache_len': Value=170\n",
      "Dict 'kv_cache':\n",
      "    Tensor 'kv_cache[k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "    Tensor 'kv_cache[v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "Bool 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 128, 96]), dtype: torch.float32, device: mps:0, min/max: -0.048/0.050\n",
      "Dict 'output[1]':\n",
      "    Tensor 'output[1][k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -2.353/2.478\n",
      "    Tensor 'output[1][v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -2.272/2.278\n",
      "====================Exiting Layer.attn_connect====================\n",
      "\n",
      "====================Entering Layer.mlp_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 128, 96]), dtype: torch.float32, device: mps:0, min/max: -4.305/4.110\n",
      "Bool 'training': Value=False\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 128, 96]), dtype: torch.float32, device: mps:0, min/max: -0.505/0.432\n",
      "====================Exiting Layer.mlp_connect====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 128, 96]), dtype: torch.float32, device: mps:0, min/max: -4.305/4.040\n",
      "Dict 'output[1]':\n",
      "    Tensor 'output[1][k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -2.353/2.478\n",
      "    Tensor 'output[1][v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -2.272/2.278\n",
      "====================Exiting Layer.forward====================\n",
      "CPU times: user 89.2 ms, sys: 10.1 ms, total: 99.4 ms\n",
      "Wall time: 144 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# INFERENCE\n",
    "module = Layer(cfg).to(cfg.device)\n",
    "print(\"number of parameters: %.2fK\" % (module.get_num_params()/1e3,))\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "#module.disable_function_logging('attn_connect')\n",
    "#module.disable_function_logging('mlp_connect')\n",
    "#module.pre_attn_norm.enable_logging()\n",
    "#module.attn.enable_logging()\n",
    "#module.post_attn_norm.enable_logging()\n",
    "#module.pre_mlp_norm.enable_logging()\n",
    "#module.mlp.enable_logging()\n",
    "#module.post_mlp_norm.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "# precompute RoPE frequencies, causal mask, and dummy input data\n",
    "if cfg.pos_enc_type == 'RoPE':\n",
    "    precompute_freqs = PrecomputeRotaryFrequencies(cfg.head_dim, cfg.max_seq_len, cfg.theta, cfg.device)\n",
    "    freqs = precompute_freqs()\n",
    "else:\n",
    "    freqs = None\n",
    "mask = torch.ones(cfg.max_seq_len, cfg.max_seq_len, dtype=torch.bool, device=cfg.device).triu(diagonal=1)\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "# setting up for kv caching\n",
    "cache_len = cfg.max_seq_len // 3\n",
    "context_chunk_len = cfg.max_seq_len // 4\n",
    "seq_len = cache_len + context_chunk_len\n",
    "kv_cache = {\n",
    "    'k': torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device),\n",
    "    'v': torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device)\n",
    "}\n",
    "# need to extend the mask with zeros for the cached values\n",
    "mask = torch.nn.functional.pad(mask[:context_chunk_len, :context_chunk_len], (cache_len, 0, 0, 0), value=False).bool()\n",
    "# these don't use seq_len because those entries should already be in the kv cache\n",
    "#freqs_cis = freqs_cis[:context_chunk_len]\n",
    "x = torch.randn(tcfg.micro_batch_size,context_chunk_len,cfg.dim).to(cfg.device)\n",
    "\n",
    "output = module(x, freqs, mask, cache_len, kv_cache)\n",
    "module.disable_logging()\n",
    "del module, freqs, mask, cache_len, context_chunk_len, seq_len, kv_cache, x, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c677ac2b-06d0-4895-b718-2bc664613c98",
   "metadata": {},
   "source": [
    "# Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "655a1fec-4e32-4c7a-86e0-1390a03e88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ccc88d4-c650-43d3-85ee-e68152cb2e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 1.01M\n",
      "Model(\n",
      "  (precompute_freqs): PrecomputeRotaryFrequencies()\n",
      "  (token_embedder): Embedding(2048, 96)\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x Layer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): SelfAttention(\n",
      "        (Wq): Linear(in_features=96, out_features=96, bias=False)\n",
      "        (Wk): Linear(in_features=96, out_features=48, bias=False)\n",
      "        (Wv): Linear(in_features=96, out_features=48, bias=False)\n",
      "        (Wo): Linear(in_features=96, out_features=96, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wup): Linear(in_features=96, out_features=256, bias=False)\n",
      "        (Wgate): Linear(in_features=96, out_features=256, bias=False)\n",
      "        (Wdown): Linear(in_features=256, out_features=96, bias=False)\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (output): Linear(in_features=96, out_features=2048, bias=False)\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "\n",
      "====================Entering Model.forward====================\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([8, 512]), dtype: torch.int64, device: mps:0, min/max: 0.000/511.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Tensor 'target_token_ids' shape: torch.Size([8, 512]), dtype: torch.int64, device: mps:0, min/max: 0.000/511.000\n",
      "\n",
      "====================Entering Layer.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.851/0.936\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([512, 512]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "====================Entering Layer.attn_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.851/0.936\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([512, 512]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.034/0.038\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.attn_connect====================\n",
      "\n",
      "====================Entering Layer.mlp_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.851/0.937\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.009/0.009\n",
      "====================Exiting Layer.mlp_connect====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.851/0.938\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.forward====================\n",
      "\n",
      "====================Entering Layer.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.851/0.938\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([512, 512]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "====================Entering Layer.attn_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.851/0.938\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([512, 512]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.026/0.034\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.attn_connect====================\n",
      "\n",
      "====================Entering Layer.mlp_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.852/0.939\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.009/0.009\n",
      "====================Exiting Layer.mlp_connect====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.851/0.940\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.forward====================\n",
      "\n",
      "====================Entering Layer.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.851/0.940\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([512, 512]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "====================Entering Layer.attn_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.851/0.940\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([512, 512]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.040/0.039\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.attn_connect====================\n",
      "\n",
      "====================Entering Layer.mlp_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.851/0.940\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.009/0.010\n",
      "====================Exiting Layer.mlp_connect====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.851/0.942\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.forward====================\n",
      "\n",
      "====================Entering Layer.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.851/0.942\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([512, 512]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "====================Entering Layer.attn_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.851/0.942\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([512, 512]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.041/0.039\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.attn_connect====================\n",
      "\n",
      "====================Entering Layer.mlp_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.851/0.942\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.008/0.008\n",
      "====================Exiting Layer.mlp_connect====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.849/0.942\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.forward====================\n",
      "\n",
      "====================Entering Layer.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.849/0.942\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([512, 512]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "====================Entering Layer.attn_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.849/0.942\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([512, 512]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.045/0.040\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.attn_connect====================\n",
      "\n",
      "====================Entering Layer.mlp_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.849/0.942\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.008/0.009\n",
      "====================Exiting Layer.mlp_connect====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.850/0.939\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.forward====================\n",
      "\n",
      "====================Entering Layer.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.850/0.939\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([512, 512]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "====================Entering Layer.attn_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.850/0.939\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([512, 512]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.040/0.033\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.attn_connect====================\n",
      "\n",
      "====================Entering Layer.mlp_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.850/0.940\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.008/0.008\n",
      "====================Exiting Layer.mlp_connect====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.848/0.943\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.forward====================\n",
      "\n",
      "====================Entering Layer.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.848/0.943\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([512, 512]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "====================Entering Layer.attn_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.848/0.943\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([512, 512]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.034/0.036\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.attn_connect====================\n",
      "\n",
      "====================Entering Layer.mlp_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.849/0.941\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.008/0.009\n",
      "====================Exiting Layer.mlp_connect====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.848/0.944\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.forward====================\n",
      "\n",
      "====================Entering Layer.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.848/0.944\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([512, 512]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "====================Entering Layer.attn_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.848/0.944\n",
      "Dict 'freqs':\n",
      "    Tensor 'freqs[cos]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "    Tensor 'freqs[sin]' shape: torch.Size([1, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -1.000/1.000\n",
      "Tensor 'mask' shape: torch.Size([512, 512]), dtype: torch.bool, device: mps:0, min/max: 0.000/1.000\n",
      "Other-type 'cache_len': Type=NoneType, Value=None\n",
      "Other-type 'kv_cache': Type=NoneType, Value=None\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.045/0.043\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.attn_connect====================\n",
      "\n",
      "====================Entering Layer.mlp_connect====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.848/0.944\n",
      "Bool 'training': Value=True\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.009/0.008\n",
      "====================Exiting Layer.mlp_connect====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.846/0.949\n",
      "Other-type 'output[1]': Type=NoneType, Value=None\n",
      "====================Exiting Layer.forward====================\n",
      "\n",
      "====================Entering Norm.forward====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.846/0.949\n",
      "\n",
      "====================Entering Norm.RMSNorm====================\n",
      "Inputs:\n",
      "Tensor 'x' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -0.846/0.949\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -4.054/4.550\n",
      "====================Exiting Norm.RMSNorm====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output' shape: torch.Size([8, 512, 96]), dtype: torch.float32, device: mps:0, min/max: -4.054/4.550\n",
      "====================Exiting Norm.forward====================\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 512, 2048]), dtype: torch.float32, device: mps:0, min/max: -1.011/2.257\n",
      "Tensor 'output[1]' shape: torch.Size([]), dtype: torch.float32, device: mps:0, min/max: 7.644/7.644\n",
      "====================Exiting Model.forward====================\n",
      "tensor(7.6436, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "CPU times: user 211 ms, sys: 40 ms, total: 251 ms\n",
      "Wall time: 382 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TRAINING\n",
    "module = Model(cfg).to(cfg.device)\n",
    "print(\"number of parameters: %.2fM\" % (module.get_num_params()/1e6,))\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "### enabling printing for sub-modules\n",
    "#module.precompute_freqs.enable_logging() # only un-comment this line if using RoPE\n",
    "#module.layers[0].enable_logging()\n",
    "for i in range(cfg.num_layers):\n",
    "    module.layers[i].enable_logging()\n",
    "module.final_norm.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "input_token_ids = torch.randint(tokenizer.vocab_len, (tcfg.micro_batch_size, cfg.max_seq_len)).to(cfg.device)\n",
    "target_token_ids = torch.randint(tokenizer.vocab_len, (tcfg.micro_batch_size, cfg.max_seq_len)).to(cfg.device)\n",
    "\n",
    "output, loss = module(input_token_ids, target_token_ids=target_token_ids)\n",
    "print(loss)\n",
    "del module, input_token_ids, target_token_ids, output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "316598a6-1ba9-4a26-962f-16c0802a698e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 1.01M\n",
      "Model(\n",
      "  (precompute_freqs): PrecomputeRotaryFrequencies()\n",
      "  (token_embedder): Embedding(2048, 96)\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x Layer(\n",
      "      (pre_attn_norm): Norm()\n",
      "      (attn): SelfAttention(\n",
      "        (Wq): Linear(in_features=96, out_features=96, bias=False)\n",
      "        (Wk): Linear(in_features=96, out_features=48, bias=False)\n",
      "        (Wv): Linear(in_features=96, out_features=48, bias=False)\n",
      "        (Wo): Linear(in_features=96, out_features=96, bias=False)\n",
      "      )\n",
      "      (pre_mlp_norm): Norm()\n",
      "      (mlp): MLP(\n",
      "        (Wup): Linear(in_features=96, out_features=256, bias=False)\n",
      "        (Wgate): Linear(in_features=96, out_features=256, bias=False)\n",
      "        (Wdown): Linear(in_features=256, out_features=96, bias=False)\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_norm): Norm()\n",
      "  (output): Linear(in_features=96, out_features=2048, bias=False)\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n",
      "\n",
      "====================Entering Model.forward====================\n",
      "Inputs:\n",
      "Tensor 'input_token_ids' shape: torch.Size([8, 128]), dtype: torch.int64, device: mps:0, min/max: 0.000/511.000\n",
      "Integer 'cache_len': Value=170\n",
      "List 'kv_cache':\n",
      "    Dict 'kv_cache[0]':\n",
      "        Tensor 'kv_cache[0][k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "        Tensor 'kv_cache[0][v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "    Dict 'kv_cache[1]':\n",
      "        Tensor 'kv_cache[1][k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "        Tensor 'kv_cache[1][v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "    Dict 'kv_cache[2]':\n",
      "        Tensor 'kv_cache[2][k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "        Tensor 'kv_cache[2][v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "    Dict 'kv_cache[3]':\n",
      "        Tensor 'kv_cache[3][k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "        Tensor 'kv_cache[3][v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "    Dict 'kv_cache[4]':\n",
      "        Tensor 'kv_cache[4][k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "        Tensor 'kv_cache[4][v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "    Dict 'kv_cache[5]':\n",
      "        Tensor 'kv_cache[5][k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "        Tensor 'kv_cache[5][v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "    Dict 'kv_cache[6]':\n",
      "        Tensor 'kv_cache[6][k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "        Tensor 'kv_cache[6][v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "    Dict 'kv_cache[7]':\n",
      "        Tensor 'kv_cache[7][k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "        Tensor 'kv_cache[7][v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: 0.000/0.000\n",
      "Other-type 'target_token_ids': Type=NoneType, Value=None\n",
      "\n",
      "Outputs:\n",
      "Tensor 'output[0]' shape: torch.Size([8, 128, 2048]), dtype: torch.float32, device: mps:0, min/max: -0.895/2.335\n",
      "List 'output[1]':\n",
      "    Dict 'output[1][0]':\n",
      "        Tensor 'output[1][0][k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -0.852/0.811\n",
      "        Tensor 'output[1][0][v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -0.901/0.841\n",
      "    Dict 'output[1][1]':\n",
      "        Tensor 'output[1][1][k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -0.823/0.789\n",
      "        Tensor 'output[1][1][v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -0.795/0.804\n",
      "    Dict 'output[1][2]':\n",
      "        Tensor 'output[1][2][k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -0.836/0.916\n",
      "        Tensor 'output[1][2][v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -0.844/0.830\n",
      "    Dict 'output[1][3]':\n",
      "        Tensor 'output[1][3][k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -0.797/0.726\n",
      "        Tensor 'output[1][3][v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -0.759/0.850\n",
      "    Dict 'output[1][4]':\n",
      "        Tensor 'output[1][4][k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -0.757/0.899\n",
      "        Tensor 'output[1][4][v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -0.738/0.738\n",
      "    Dict 'output[1][5]':\n",
      "        Tensor 'output[1][5][k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -0.848/0.913\n",
      "        Tensor 'output[1][5][v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -0.807/0.864\n",
      "    Dict 'output[1][6]':\n",
      "        Tensor 'output[1][6][k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -0.894/0.847\n",
      "        Tensor 'output[1][6][v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -0.827/0.694\n",
      "    Dict 'output[1][7]':\n",
      "        Tensor 'output[1][7][k]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -0.842/0.841\n",
      "        Tensor 'output[1][7][v]' shape: torch.Size([8, 512, 1, 48]), dtype: torch.float32, device: mps:0, min/max: -0.979/0.753\n",
      "====================Exiting Model.forward====================\n",
      "CPU times: user 68.6 ms, sys: 18.8 ms, total: 87.3 ms\n",
      "Wall time: 93.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Inference\n",
    "module = Model(cfg).to(cfg.device)\n",
    "print(\"number of parameters: %.2fM\" % (module.get_num_params()/1e6,))\n",
    "print(module)\n",
    "\n",
    "module.enable_logging()\n",
    "### enabling printing for sub-modules\n",
    "#for i in range(cfg.num_layers):\n",
    "    #module.layers[i].enable_logging()\n",
    "#module.final_norm.enable_logging()\n",
    "\n",
    "# optionally enabling printing of every single input/output tensor\n",
    "#module.enable_full_tensor_printing()\n",
    "\n",
    "input_token_ids = torch.randint(tokenizer.vocab_len, (tcfg.micro_batch_size, cfg.max_seq_len // 4)).to(cfg.device)\n",
    "kv_cache = [{ # Initialize kv caches for each layer\n",
    "                \"k\": torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device),\n",
    "                \"v\": torch.zeros((tcfg.micro_batch_size, cfg.max_seq_len, cfg.num_kv_heads, cfg.head_dim), device=cfg.device),\n",
    "            } for _ in range(cfg.num_layers)]\n",
    "\n",
    "output, kv_cache = module(input_token_ids, cache_len = cfg.max_seq_len // 3, kv_cache = kv_cache)\n",
    "\n",
    "del module, input_token_ids, kv_cache, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18f272a-9e60-4d41-9fe4-01a97099a4c9",
   "metadata": {},
   "source": [
    "# Learning Rate Schedule Display\n",
    "thought i'd make somewhere to help you visualize what the learning rate schedule settings you've got look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08801c2c-6f0f-4f41-a4de-25d8b10e2ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from train import scheduler_lambda\n",
    "from config import TrainConfig\n",
    "tcfg = TrainConfig()\n",
    "    \n",
    "# Generate learning rate values\n",
    "lrs = [scheduler_lambda(i) for i in range(tcfg.max_iters)]\n",
    "    \n",
    "# Plot the learning rates\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(lrs, label='Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd1f69f-20f3-4088-a022-1a82d9f21048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
